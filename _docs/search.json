[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ITS_LIVE_TOOL",
    "section": "",
    "text": "ITS_LIVE_TOOL is a package designed to aid users working with the Inter-mission Time Series of Land Ice Velocity and Elevation (ITS_LIVE) dataset. The package provides functions for accessing data as well as various methods to process ITS_LIVE observations. This notebook will demonstrate various elements of the package and walk through the steps of a typical workflow using ITS_LIVE_TOOL.\n\n\nkeep this overview here? or move to readme doc\nThe tools in the package fall into two main categories: 1) data access + organization, 2) data processing.\n\n\n\n\nThis is an interactive widget design to streamline access ITS_LIVE image pair ice velocity time series.\n\n\n\nThese are provided to store and keep track of different types of data related to individual units of analysis such as points, centerlines or full glacier surface areas.\nThis roadmap document will first demonstrate data access and organization tools before demonstrating the processing tools.\n\n\n\n\nWe demonstrate and make available two processing routines. Be sure to check out the accompanying book and consider if either of these are appropriate for your data and use case. Note that these methods are in active development and thus should be considered in beta phase. Please perform your own data inspection and due diligence if implementing these methods.\n\n\nto add Description – link to full description and examples\n\n\n\nto add Description – link to full description and examples\n\n\n\n\nWe demonstrate and make available two processing routines. Be sure to check out the accompanying book and consider if either of these are appropriate for your data and use case. Note that these methods are in active development and thus should be considered in beta phase. Please perform your own data inspection and due diligence if implementing these methods.\n\nfrom ITS_LIVE_TOOL import datacube_tools, interactive, obj_setup\n\n\nimport os\nimport numpy as np\nimport pyproj\nimport matplotlib.path as path\nimport s3fs\nimport zarr\nimport matplotlib.pyplot as plt\nimport scipy\nfrom datetime import timedelta\nfrom tqdm import tqdm\nimport xarray as xr\nimport re\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.path as mplp\nimport ipyleaflet as ipyl\nfrom ipyleaflet import WMSLayer\nimport ipywidgets as ipyw\nimport json\nimport pandas as pd\nfrom ipyleaflet import Map, WMSLayer, basemaps\nfrom ipywidgets import HTML\nfrom owslib.wms import WebMapService\n\n\n\n\n\nInstall this package using the below command\nsomeday we hope to have a pip or conda install, for now use pip install git+ github repo url"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "ITS_LIVE_TOOL",
    "section": "",
    "text": "keep this overview here? or move to readme doc\nThe tools in the package fall into two main categories: 1) data access + organization, 2) data processing.\n\n\n\n\nThis is an interactive widget design to streamline access ITS_LIVE image pair ice velocity time series.\n\n\n\nThese are provided to store and keep track of different types of data related to individual units of analysis such as points, centerlines or full glacier surface areas.\nThis roadmap document will first demonstrate data access and organization tools before demonstrating the processing tools.\n\n\n\n\nWe demonstrate and make available two processing routines. Be sure to check out the accompanying book and consider if either of these are appropriate for your data and use case. Note that these methods are in active development and thus should be considered in beta phase. Please perform your own data inspection and due diligence if implementing these methods.\n\n\nto add Description – link to full description and examples\n\n\n\nto add Description – link to full description and examples\n\n\n\n\nWe demonstrate and make available two processing routines. Be sure to check out the accompanying book and consider if either of these are appropriate for your data and use case. Note that these methods are in active development and thus should be considered in beta phase. Please perform your own data inspection and due diligence if implementing these methods.\n\nfrom ITS_LIVE_TOOL import datacube_tools, interactive, obj_setup\n\n\nimport os\nimport numpy as np\nimport pyproj\nimport matplotlib.path as path\nimport s3fs\nimport zarr\nimport matplotlib.pyplot as plt\nimport scipy\nfrom datetime import timedelta\nfrom tqdm import tqdm\nimport xarray as xr\nimport re\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.path as mplp\nimport ipyleaflet as ipyl\nfrom ipyleaflet import WMSLayer\nimport ipywidgets as ipyw\nimport json\nimport pandas as pd\nfrom ipyleaflet import Map, WMSLayer, basemaps\nfrom ipywidgets import HTML\nfrom owslib.wms import WebMapService"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "ITS_LIVE_TOOL",
    "section": "",
    "text": "Install this package using the below command\nsomeday we hope to have a pip or conda install, for now use pip install git+ github repo url"
  },
  {
    "objectID": "index.html#gaussian-process-regression-1",
    "href": "index.html#gaussian-process-regression-1",
    "title": "ITS_LIVE_TOOL",
    "section": "Gaussian Process Regression",
    "text": "Gaussian Process Regression\nWe demonstrate a method to develop and apply a Gaussian Process model on ITS_LIVE point time series data. Gaussian Process are a non-parametric, Bayesion regression method to model a latent function such as glacier surface velocity with respect to time, given observations of ice movement over different time periods.\nThis method is in active development and should currently be considered to be in 'beta' mode. We provide an example of its implementation but stress that it may not be suitable to apply in other scenarios. Please carefully inspect your dataset and the model parameters to be sure it is appropriate before using in your workflows\n\nProblem:\nITS_LIVE image pair ice velocity time series provide estimates of ice movement over a broad range of temporal baselines (number of days between image pairs). The estimates describe different types of ice movement given different temporal baselines and contain noise inherent to sensor algorithmic limitations. Given this complex, noisy dataset, we would like to employ a regression approach to reach an estimate of an underlying velocity function with inherent uncertainty quantifications.\n\n\nRaw data\n\n\n\nITS_LIVE velocity observations for a single point\n\n\n\n\nGaussian Process model predictions\n\n\n\nVelocity time series predicted by Gaussian Process model based on ITS_LIVE velocity observations\n\n\nPut inversion and other processing steps here?"
  },
  {
    "objectID": "index.html#select-your-data",
    "href": "index.html#select-your-data",
    "title": "ITS_LIVE_TOOL",
    "section": "Select your data",
    "text": "Select your data\n\nInstructions:\n\nRight-click on a glacier to get its RGI ID. It should illuminate in blue once you have selected it. You can select multiple glaciers.\nWARNING: if you are on a web browser (Cryocloud, OpenScienceLab, etc…): right-click will open a pop-up menu. Simply left-click somewhere on your screen to make it disappear.\nLeft-click on the datacube (grey overlay, red when you hover your mouse over it) you want to download. You can select multiple datacubes. When a datacube URL has been fetched, it should print it below the GUI.\n\n\nurls = []\n\n\ndata_map = interactive.Widget()\n\n\ndata_map.display()\n\n\n\n\n\ntry: \n    coords, gpdf, urls = interactive.return_clicked_info(data_map)\nexcept:\n    pass\n\n1\n\n\n\n##### EMMA IN NOTEBOOK 2 ADD 'GeoData' to the import line: from ipyleaflet import Map, WMSLayer, basemaps, GeoData\n\nt = []\nwms_url = \"https://glims.org/geoserver/ows?SERVICE=WMS&\"\nwms_layer = WMSLayer(\n    url=wms_url,\n    layers='GLIMS:RGI',\n    transparent=True,\n    format='image/png'\n)\n\n\n# Create a Pandas DataFrame with X and Y coordinates\n#data = pd.DataFrame({'X': list(fx_map[::50]), 'Y': list(fy_map[::50])})\n\n# Map and label widgets\nmap = ipyl.Map(basemap=basemaps.Esri.WorldImagery, center=(0, 0), zoom=2)\nlabel = ipyw.Label(layout=ipyw.Layout(width=\"100%\"))\n\n# Create a list to store clicked URLs\nurls = []\n\n# geojson layer with hover handler\nwith open(\"catalog_v02.json\") as f:\n    geojson_data = json.load(f)\n\nfor feature in geojson_data[\"features\"]:\n    feature[\"properties\"][\"style\"] = {\n        \"color\": \"grey\",\n        \"weight\": 1,\n        \"fillColor\": \"grey\",\n        \"fillOpacity\": 0.5,\n    }\n\ngeojson_layer = ipyl.GeoJSON(data=geojson_data, hover_style={\"fillColor\": \"red\"}, selected_style = {\"fillColor\": \"orange\"})\n\ndef hover_handler(event=None, feature=None, id=None, properties=None):\n    label.value = properties[\"zarr_url\"]\n\ndef json_handler(event=None, feature=None, id=None, properties=None):    \n    zarr_url = properties.get(\"zarr_url\", \"N/A\")\n    urls.append(zarr_url)\n    print(f\"Clicked URL: {zarr_url}\")\n    print(\"All Clicked URLs:\", urls)\n    '''\n    # Plot in orange the polygon clicked on\n    json_shape = [{'type': properties['geometry_epsg']['type'], 'coordinates': properties['geometry_epsg']['coordinates']}]\n    geom = [shape(i) for i in json_shape]\n    json_shape = gpd.GeoDataFrame({'geometry':geom})  \n    json_shape = GeoData(geo_dataframe = json_shape,\n                style={'color': 'black', 'fillColor': '#3366cc', 'opacity':0.05, 'weight':1.9, 'dashArray':'2', 'fillOpacity':0.6},\n                hover_style={'fillColor': 'blue' , 'fillOpacity': 0.2},\n                name = 'Cube')\n\n    map.add_layer(json_shape)\n    '''\n\n    \n    \nwms = WebMapService(wms_url)\n\n# Initialize a list to store the geopandas dataframes\ngdf_list = []\n\n# Create a function to handle click events\ndef click_handler(properties=None, **kwargs):\n\n\n    if kwargs.get('type') == 'contextmenu':\n        latlon = kwargs.get('coordinates')\n        lat, lon = latlon[0], latlon[1]\n        print(f\"Clicked at (Lat: {lat}, Lon: {lon})\")\n        \n        # Arrange the coordinates\n        \n        response = wms.getfeatureinfo(\n            layers=['GLIMS:RGI'],\n            srs='EPSG:4326',\n            bbox=(lon-0.001,lat-0.001,lon+0.001,lat+0.001),\n            size=(1,1),\n            format='image/jpeg',\n            query_layers=['GLIMS:RGI'],\n            info_format=\"application/json\",\n            xy=(0,0))\n        df = gpd.read_file(response)\n        gdf_list.append(df)  \n        try:\n            print(f\"You have selected the glacier {df['NAME'].values[0]}, ID: {df['id'].values[0]} \")\n        except:\n            print(f\"This glacier is not recognized by the RGI (maybe an ice-shelf ?) -&gt; Choose another one\")\n        geo_data = GeoData(geo_dataframe = df,\n                   style={'color': 'black', 'fillColor': '#3366cc', 'opacity':0.05, 'weight':1.9, 'dashArray':'2', 'fillOpacity':0.6},\n                   hover_style={'fillColor': 'blue' , 'fillOpacity': 0.2},\n                   name = 'Glacier')\n\n        gdf_list.append(df) \n        map.add_layer(geo_data)\n\n        \n\n\ngeojson_layer.on_hover(hover_handler)\ngeojson_layer.on_click(json_handler)\n\n\nmap.add(geojson_layer)\n\n# Add points from the DataFrame to the map\n#for index, row in data.iterrows():\n#    marker = ipyl.Marker(locatfairbanks ash situationion=(row['Y'], row['X']))\n#    map.add_layer(marker)\n\n# Enable zoom when scrolling with the mouse\nmap.scroll_wheel_zoom = True\nipyw.VBox([map, label])\n\n\n# Add the WMS layer to the map\nmap.add_layer(wms_layer)\n\n# Add the click event handler to the map\nmap.on_interaction(click_handler)\n\nmap\n\nThe following cell ensures you don’t have duplicate glaciers in your selection. If so, it will automatically remove them. It also verifies that you have selected a datacube to download. If not, please re-select it and ensure you are getting a URL after a left-click.\n\n### EMMA THIS IS THE CELL TO MODIFY\n\n# Get indices of unique geopandas dataframes in case you selected the same glacier twice\nunique_values, unique_indices = np.unique(np.array([i['id'] for i in gdf_list]), return_index=True)\n\n# Iterate through the unique indices to recreate the list of geopandas dataframes unique only\ngdf_list = [gdf_list[i] for i in unique_indices]\n\nprint(f\"You have {len(gdf_list)} glaciers selected\")\n\nif len(urls) == 0:\n    print('Select a datacube to fetch the data !!')\n\n# Print the first dataframe to show the structure of it\ngdf_list[0]\n\nJust a peek at what the Randolph Glacier Inventory (RGI) gives us for a glacier object:\n\ngpdf[0][0]\n\n\nurls\n\n['http://its-live-data.s3.amazonaws.com/datacubes/v2/N20E080/ITS_LIVE_vel_EPSG32644_G0120_X750000_Y3250000.zarr']\n\n\n\ntry: \n    gpdf_ls = gpdf[0]\nexcept:\n    pass\n\n\ndef make_input_dict(coords, gpdf, urls):\n    \n    mod_urls = [re.sub(r'http', 's3', url) for url in urls]\n    mod_urls = [re.sub(r'\\.s3\\.amazonaws\\.com', '', url) for url in mod_urls]\n    \n    d = {'coords': coords,\n     'gpdf': gpdf,\n     'urls': mod_urls\n        }\n    return d\n\n\ntry:\n    d = make_input_dict(coords, gpdf, URLs)\n    urls = d['urls']\nexcept:\n    print(' no input data specified, have you made a map selection?')\n\n no input data specified, have you made a map selection?\n\n\nThe following function creates a dictionnary for a datacube URL. It prepares the many variables we will use during the cube inversion.\n\ndef create_data_dict(urls):\n    # Modify the urls so they can be opened by zarr (replace 'http' by 's3' and delete '.s3.amazonaws.com')\n    mod_urls = [re.sub(r'http', 's3', url) for url in urls]\n    mod_urls = [re.sub(r'\\.s3\\.amazonaws\\.com', '', url) for url in urls]\n    \n    # Create storing arrays for the coordinates on-glacier\n    X_valid = []\n    Y_valid = []\n    X_tot = []\n    Y_tot = []\n    \n    # Create an empty directoryimport pickle to hold many variables all tied to the datacubes\n    data_dict = {}\n    \n\n    # We iterate through the different datacubes so they can each have one instance of the variables below\n    for mod_urls in urls:\n        zarr_store = None # To store the datacube's information and access its variables\n        dates = None # To store the dates at which the inversion will give values\n        A_m = None # 1st part of the design matrix\n        reg_mat_Inv = None # Regularization in time, 2nd part of the design matrix\n        mission = None # If you want to invert specifically for one mission in particular ('S1','L8','L9', etc...)\n        index_sort = None # Indices representing the sorted dates (from older to most recent)\n        inds_mission = None # Indices representing the sorted dates per mission chosen\n        ind_tot = None # Indices representing the indices of the pixels on the GOI\n        valid_idx = None # Easting and Northing values of the indices above\n        proj_cube = None # Projection of the datacube\n        mask_dates = None # Mask that filters out dates outside of desired date range\n        \n        # Create a dictionary entry for the URL with the desired subsets\n        data_dict[mod_urls] = {\n            'zarr_store': zarr_store,\n            'dates_noinv': dates,\n            'A_m': A_m,\n            'reg_mat_Inv': reg_mat_Inv,\n            'mission': mission,\n            'index_sort': index_sort,\n            'inds_mission': inds_mission,\n            'dates': dates,\n            'ind_tot': ind_tot,\n            'valid_idx': valid_idx,\n            'proj_cube': proj_cube,\n            'mask_dates': mask_dates\n        }\n        \n    return data_dict, X_valid, Y_valid, X_tot, Y_tot\n\nThis function inputs a datacube. It fetches its spatial footprint, and flags which cells that fall on the glacier shape you selected earlier. The purpose is to create an array that gathers the spatial intersection between all the datacubes and glaciers you selected. It also creates a mask so we only download datacube cells on a glacier, and we don’t compute the inversion on non-glacier cells in case some get through the filter.\n\ndef get_extents(url, gpdf, X_tot, Y_tot, X_valid, Y_valid, data_dict):# mission, lamb, derivative, day_interval):\n    \n    #url = input_data_dict['urls'].iloc[0]\n    \n    # Open the zarr files\n    fs = s3fs.S3FileSystem(anon=True)\n    store = zarr.open(s3fs.S3Map(url, s3=fs))\n   \n    # Update the dictionnary\n    data_dict[url]['zarr_store'] = store\n\n    # Get the cube's projection\n    proj_cube = store.attrs['projection']\n\n    # Load X and Y of the dataset\n    X = store['x'][:]\n    Y = store['y'][:]\n\n    # Store the arrays in the total list\n    X_tot.append(X)\n    Y_tot.append(Y)\n\n    # Load dimensions\n    shape_arr = store['v'].shape\n    \n    Xs, Ys = np.meshgrid(X, Y)\n    points = np.array((Xs.flatten(), Ys.flatten())).T\n\n    idx_valid = []\n    \n    for b in range(len(gpdf)):\n        mpath = mplp.Path(list(gpdf[b]['geometry'].to_crs(np.int(proj_cube)).boundary.explode(index_parts = True).iloc[0].coords))\n        glacier_mask = mpath.contains_points(points).reshape(Xs.shape)\n        # Grab the indices of the points inside the glacier\n        idx_valid.append(np.array(np.where(glacier_mask==True)))\n        \n    idx_valid = np.hstack(idx_valid)\n    # Store the valid indices\n    data_dict[url]['valid_idx'] = idx_valid\n    \n    # Store the cube projection\n    data_dict[url]['proj_cube'] = proj_cube\n    \n    # Store the coordinates of the valid Xs and Ys\n    X_valid.append([Xs[idx_valid[0][i], idx_valid[1][i]] for i in range(len(idx_valid[0]))])\n    Y_valid.append([Ys[idx_valid[0][i], idx_valid[1][i]] for i in range(len(idx_valid[0]))])\n    \n    return X_tot, Y_tot, X_valid, Y_valid\n\nFor each datacube you selected, locate the on-glacier cells. Once it has been done, create an array gathering all the glacier cells you want to invert.\n\ntry:\n    data_dict, X_valid, Y_valid, X_tot, Y_tot = create_data_dict(urls)\n\n    for url in tqdm(range(len(urls))):\n        X_tot, Y_tot, X_valid, Y_valid = get_extents(urls[url], gpdf_ls, X_tot, Y_tot, X_valid, Y_valid, data_dict)\n\n    # Create Eastings and Northings arrays based on the Eastings and Northings of the datacubes\n    X_arr = np.unique(np.hstack(X_tot))\n    Y_arr = np.unique(np.hstack(Y_tot))\n    \n    # Crop to the GOI (so we avoid over-filling our matrix with NaNs)\n    x_min = np.where(np.min(np.hstack(X_valid)) == X_arr)[0][0]\n    x_max = np.where(np.max(np.hstack(X_valid)) == X_arr)[0][0]\n    y_min = np.where(np.min(np.hstack(Y_valid)) == Y_arr)[0][0]\n    y_max = np.where(np.max(np.hstack(Y_valid)) == Y_arr)[0][0]\n    \n    \n    # And now search the indices corresponding to the coordinates \n    x_matches = np.hstack([[np.where(i == X_arr[min(x_min-1, x_max+1):max(x_min-1, x_max+1)])[0][0] for i in row] for row in X_valid]).astype(int)\n    y_matches = np.hstack([[np.where(i == Y_arr[min(y_min-1, y_max+1):max(y_min-1, y_max+1)])[0][0] for i in row] for row in Y_valid]).astype(int)\n    \n    # Create an array representing the glacier\n    template = np.zeros((len(Y_arr[min(y_min-1, y_max+1):max(y_min-1, y_max+1)]), len( X_arr[min(x_min-1, x_max+1):max(x_min-1, x_max+1)])))\n    template[y_matches, x_matches] = 1\nexcept:\n    pass\n\n0it [00:00, ?it/s]\n\n\nPlot the glacier shape to visualize which cells will be inverted/downloaded.\n\ntry:\n    plt.pcolormesh(X_arr[x_min-1:x_max+1], Y_arr[y_min-1:y_max+1], template)\nexcept:\n    pass\n\nDesign Matrix\nThe following function creates a unique design matrix for each datacube. This design matrix will then be assembled in the inversion function. The design matrix shares a common base for each point in the same datacube. But because each point has a different temporal completness, we modify the base design matrix to fit the point’s data.\n\ndef design_matrices(url, mission, lamb, derivative, day_interval, sdate, edate):\n\n    # If you passed 'mission' as an argument, it grabs the appropriate values\n    if mission:\n        # Get the indices of the mission\n        filt1 = np.where(data_dict[urls[url]]['zarr_store']['satellite_img1'][:] == mission)\n        filt2 = np.where(data_dict[urls[url]]['zarr_store']['satellite_img2'][:] == mission)\n        inds_mission = np.intersect1d(filt1[0],filt2[0])\n\n        # Grab only the indices corresponding to the missions\n        mid_dates = np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['mid_date'][:], dtype='timedelta64[D]')[inds_mission]\n        im1 = np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['acquisition_date_img1'][:], dtype='timedelta64[D]')[inds_mission]\n        im2 = np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['acquisition_date_img2'][:], dtype='timedelta64[D]')[inds_mission]\n    else:\n        # If 'None' was passed as a mission argument, we grab all the available data.\n        inds_mission = None\n        mid_dates = np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['mid_date'][:], dtype='timedelta64[D]')\n        im1 = np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['acquisition_date_img1'][:], dtype='timedelta64[D]')\n        im2 = np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['acquisition_date_img2'][:], dtype='timedelta64[D]')\n    \n    # Get some arrays\n    index_sort = np.argsort(np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['mid_date'][:], dtype='timedelta64[D]'))\n    mid_dates = mid_dates[index_sort]\n    im1 = im1[index_sort]\n    im2 = im2[index_sort]\n\n    # If sdate is later than the first available date, we find its corresponding index\n    try:\n        sdate_ind = np.where(mid_dates &gt;= sdate)[0][0]\n    except:\n        sdate_ind = 0\n    \n    # If edate is sooner than the last available date, we find its corresponding index\n    try:\n        edate_ind = np.where(mid_dates &gt; edate)[0][0]\n    except:\n        edate_ind = None\n    \n    # Create a False/True mask where True if the date is in the desired range\n    mask_dates = np.full(mid_dates.shape, False)\n    mask_dates[sdate_ind:edate_ind] = True\n\n    # Keep only the values within the desired range\n    mid_dates = mid_dates[mask_dates]\n    im1 = im1[mask_dates]\n    im2 = im2[mask_dates]\n\n    # Check which im is the smallest (first image, it changes depending on ITS_LIVE's version)\n    if im2[0] &lt; im1[0]:\n        temp = im1\n        im1 = im2\n        im2 = temp\n\n    # Create the date array with the new interval dates\n    dates_nonum = np.arange(mid_dates[0], mid_dates[-1], timedelta(days=day_interval)).astype(np.datetime64)\n\n    # Convert to numerical\n    dates = (dates_nonum - np.datetime64('1970-01-01T00:00:00Z'))/np.timedelta64(1, 's')\n    dt_start = (im1 - np.datetime64('1970-01-01T00:00:00Z'))/np.timedelta64(1, 's')\n    dt_end = (im2 - np.datetime64('1970-01-01T00:00:00Z'))/np.timedelta64(1, 's')\n\n    # --------------- DESIGN MATRICES --------------- \n\n    # Initialize matrix\n    A_m = np.zeros((mid_dates.shape[0],dates.shape[0]))\n\n    # We have to iterate through the satellite pairs that actually gave a measurement\n    for j in range(1, len(mid_dates)):\n    # current contents of your for loop\n\n        # Find the middate that is the closest to dt_start (supequal)\n        start = np.argmin(np.abs(dates-dt_start[j]))\n\n        # Find the middate that is closest to dt_end (infequal)\n        end = np.argmin(dt_end[j] - dates[dates &lt;= dt_end[j]])\n\n        # Divide 1 by the amount of middates between d_start and d_end \n        if end == A_m.shape[1]-1: # If the mid_date is at the end of the array (acquisition im2 equals last mid_date)\n            A_m[j, start:] = 1/(1+A_m.shape[1]-start)\n        else: # If the measurement is in A's bounds temporally (we can have a satellite pair with the 2nd pair being outside of our mid_dates)\n            A_m[j, start:end+1] = 1/(1+end-start) # Attribute to each pixel in the timescale of the satellite pair, the 1/amount of pixel in the pairmid_dates.shape\n\n\n    # Initialize regularization matrix\n    if derivative == 1:\n        reg_mat_Inv = np.zeros((A_m.shape[1] -1, A_m.shape[1]))\n\n        for j in range(A_m.shape[1] -1):\n            reg_mat_Inv[j, j] = -lamb/day_interval\n            reg_mat_Inv[j, j+1] = lamb/day_interval\n\n    elif derivative == 2:\n        # Initialize 2nd derivative regularization matrix\n        reg_mat_Inv = np.zeros((A_m.shape[1] -1, A_m.shape[1]))\n\n        for j in range(A_m.shape[1] -2):\n            reg_mat_Inv[j, j] = lamb/(day_interval**2)\n            reg_mat_Inv[j, j+1] = -2*lamb/(day_interval**2)\n            reg_mat_Inv[j, j+2] = lamb/(day_interval**2)\n            \n    data_dict[urls[url]]['A_m'] = A_m\n    data_dict[urls[url]]['reg_mat_Inv'] = reg_mat_Inv\n    data_dict[urls[url]]['mission'] = mission\n    data_dict[urls[url]]['index_sort'] = index_sort\n    data_dict[urls[url]]['inds_mission'] = inds_mission\n    data_dict[urls[url]]['dates'] = dates_nonum\n    data_dict[urls[url]]['dates_noinv'] = mid_dates\n    data_dict[urls[url]]['mask_dates']= mask_dates\n            \n    return data_dict\n\nCreate one design matrix per datacube\n\ntry:\n\n    for i in tqdm(range(len(urls))):\n        design_matrices(i, mission, lamb, derivative, day_interval)\nexcept:\n    pass\n\n0it [00:00, ?it/s]\n\n\nPoint Inversion\nThis function inputs a timeseries for a point and inverts it based on the design matrix build for the datacube the point belongs to.\n\ndef Inv_reg(vObs, data, fillvalue):\n    \n    # Grab observed velocities\n    vObs = vObs[data['index_sort']]\n    vObs = vObs[data['mask_dates']]\n    \n    # Filter out the missions we don't want\n    if mission:\n        vObs = vObs[data['inds_mission']]  \n    \n    # Mask the NaNs so we don't compute the inversion for empty rows\n    mask = np.logical_not(np.equal(vObs, fillvalue))\n    \n    # Create a masked velocity vector\n    vObs_masked = vObs[mask]\n    \n    # Append regularization terms to dObs\n    vObs_masked= np.hstack((vObs_masked, np.zeros((data['reg_mat_Inv'].shape[0]))))\n     \n    # Assemble the design matrix\n    A_des = np.vstack((data['A_m'][mask], data['reg_mat_Inv']))\n    \n    # Invert the velocities\n    vInv = np.linalg.solve(A_des.T@A_des,A_des.T@vObs_masked)\n    #vInv = scipy.linalg.solve(A_des.T@A_des,A_des.T@vObs_masked, lower = False, check_finite=False, assume_a='gen')\n    \n    return vInv\n\nGather the dates at which we have an observation, spanning all the selected datacubes\n\n# Get the total amount of temporal steps\nind_tot = []\nfor i in urls:\n    ind_tot.append(data_dict[i]['dates'])\n\nind_tot = np.unique(np.hstack(ind_tot))\n\nfor i in urls:\n    data_dict[i]['ind_tot'] = np.array([np.where(c == ind_tot)[0][0] for c in data_dict[urls[0]]['dates']])\n\nInversion Calculation\nThe following cell loops through all the on-ice cells in the datacubes. It runs the inversion and outputs a timeseries linearized in time (based on your input at the beginning of the notebook). It generates a .txt file that shows how many cells have been inverted. This is a check in case you get disconnected from your notebook but it still runs in the background. It also saves the output every 10000 iterations so if the script crashes, you don’t have to start from scratch.\nOnce the inversion is complete, it saves the output one last time as a netcdf file containing: vx, vy, time, x, y.\n\nif GPU:\n\n    import torch\n\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    '''\n    # Migrate to torch & GPU\n    for c in range(len(urls)):\n        data_dict[urls[c]]['A_m'] = torch.from_numpy(data_dict[urls[c]]['A_m']).to(device)\n        data_dict[urls[c]]['reg_mat_Inv'] = torch.from_numpy(data_dict[urls[c]]['reg_mat_Inv']).to(device)\n    '''\n    \n    def Inv_reg_torch(vObs, data, fillvalue, device):\n     \n        # Grab observed velocities\n        vObs = vObs[data['index_sort']]\n        vObs = vObs[data['mask_dates']]\n        \n        # Filter out the missions we don't want\n        if mission:\n            vObs = vObs[data['inds_mission']]  \n        \n        # Mask the NaNs so we don't compute the inversion for empty rows\n        mask = np.logical_not(np.equal(vObs, fillvalue))\n        \n        # Create a masked velocity vector\n        vObs_masked = torch.from_numpy(vObs[mask])\n        \n        # Append regularization terms to dObs\n        vObs_masked= torch.hstack((vObs_masked, torch.zeros((data['reg_mat_Inv'].shape[0])))).to(device)\n         \n        # Assemble the design matrix\n        A_des = torch.vstack((data['A_m'][mask], data['reg_mat_Inv']))\n        \n        # Invert the velocities\n        vInv = torch.linalg.solve(A_des.T@A_des,A_des.T@vObs_masked.double())\n        \n        return vInv\n    \n    vxInv = torch.zeros((len(ind_tot), template.shape[0], template.shape[1])).double().to(device)\n    vyInv = torch.zeros((vxInv.shape)).double().to(device)\n    \n    # Define the total number of iterations\n    total_iterations = len(y_matches)\n    \n    # Create a tqdm object with dynamic_ncols=False to suppress intermediate updates\n    # Create a tqdm object with a larger mininterval to suppress intermediate updates\n    progress_bar = tqdm(total=total_iterations, dynamic_ncols=False, mininterval=1.0)\n    \n    \n    i = 0\n    for c in range(len(urls)):\n        valid_idx = data_dict[urls[c]]['valid_idx']\n        fillvx = data_dict[urls[c]]['zarr_store']['vx'][:, valid_idx[0][0], valid_idx[1][0]].min() #data_dict[urls[c]]['zarr_store']['vx'].fill_value   fill_value is wrong in the new ITS_LIVE version\n        fillvy = data_dict[urls[c]]['zarr_store']['vx'][:, valid_idx[0][0], valid_idx[1][0]].min() #data_dict[urls[c]]['zarr_store']['vy'].fill_value\n    \n        for V in range(len(valid_idx[0])):\n            vxObs = data_dict[urls[c]]['zarr_store']['vx'][:, valid_idx[0][V], valid_idx[1][V]]\n            vyObs = data_dict[urls[c]]['zarr_store']['vy'][:, valid_idx[0][V], valid_idx[1][V]]\n            vxInv[data_dict[urls[c]]['ind_tot'], y_matches[i], x_matches[i]] = Inv_reg_torch(vxObs, data_dict[urls[c]], fillvx, device)\n            vyInv[data_dict[urls[c]]['ind_tot'], y_matches[i], x_matches[i]] = Inv_reg_torch(vyObs, data_dict[urls[c]], fillvy, device)\n            \n            i += 1\n            progress_bar.update(1)  # Update the progress bar\n\n            if i%100 == 0:\n                with open(\"Counter.txt\", \"w\") as text_file:\n                    text_file.write(f\"Counter: {i}\")\n            \n            if i%5000 == 0:\n                \n                print(f\"Saved at {i}\")\n                # Get the names of all the glaciers in the datacube \n                #glac_names = np.hstack(np.array([glacier.id.values[0] for glacier in gdf_list]))\n                #glac_names = '-'.join(glac_names)\n\n                # Gather the projection of the cube\n                #glac_proj = str(np.unique(np.hstack([data_dict[urls[i]]['proj_cube'] for i in range(len(urls))]))[0])\n\n                # Create a new dataset with vx and vy, using attributes from 'ds'\n                new_ds = xr.Dataset(\n                    {\n                        \"vx\": ([\"time\", \"y\", \"x\"], vxInv.cpu().numpy()),\n                        \"vy\": ([\"time\", \"y\", \"x\"], vyInv.cpu().numpy()),\n                    },\n                    coords={\n                        \"time\": ind_tot,\n                        \"x\": X_arr[x_min-1:x_max+1],\n                        \"y\": Y_arr[y_min-1:y_max+1],\n                    },\n                    attrs=data_dict[urls[0]]['zarr_store'].attrs,\n                ).chunk({'time': 1, 'x': 100, 'y': 100})\n\n                from dask.diagnostics import ProgressBar\n                write_job = new_ds.to_netcdf(f'VelInv.nc', compute=False)\n                with ProgressBar():\n                    print(f\"Writing to {'VelInv.nc'}\")\n                    write_job.compute()\n    \n    # Close the progress bar\n    progress_bar.close()\n\n    # Save the dataset\n    new_ds = xr.Dataset(\n        {\n            \"vx\": ([\"time\", \"y\", \"x\"], vxInv.cpu().numpy()),\n            \"vy\": ([\"time\", \"y\", \"x\"], vyInv.cpu().numpy()),\n        },\n        coords={\n            \"time\": ind_tot,\n            \"x\": X_arr[x_min-1:x_max+1],\n            \"y\": Y_arr[y_min-1:y_max+1],\n        },\n        attrs=data_dict[urls[0]]['zarr_store'].attrs,\n    ).chunk({'time': 1, 'x': 100, 'y': 100})\n    \n    from dask.diagnostics import ProgressBar\n    write_job = new_ds.to_netcdf(f'VelInv.nc', compute=False)\n    with ProgressBar():\n        print(f\"Writing to {'VelInv.nc'}\")\n        write_job.compute()\n\n\n\nelse:\n    \n    vxInv = np.zeros((len(ind_tot), template.shape[0], template.shape[1]))\n    vyInv = np.zeros((vxInv.shape))\n    \n    # Define the total number of iterations\n    total_iterations = len(y_matches)\n    \n    # Create a tqdm object with dynamic_ncols=False to suppress intermediate updates\n    # Create a tqdm object with a larger mininterval to suppress intermediate updates\n    progress_bar = tqdm(total=total_iterations, dynamic_ncols=False, mininterval=1.0)\n    \n    \n    i = 0\n    for c in range(len(urls)):\n        valid_idx = data_dict[urls[c]]['valid_idx']\n        fillvx = data_dict[urls[c]]['zarr_store']['vx'][:, valid_idx[0][0], valid_idx[1][0]].min() #data_dict[urls[c]]['zarr_store']['vx'].fill_value   fill_value is wrong in the new ITS_LIVE version\n        fillvy = data_dict[urls[c]]['zarr_store']['vx'][:, valid_idx[0][0], valid_idx[1][0]].min() #data_dict[urls[c]]['zarr_store']['vy'].fill_value\n        \n        for V in range(len(valid_idx[0])):\n            vxObs = data_dict[urls[c]]['zarr_store']['vx'][:, valid_idx[0][V], valid_idx[1][V]]\n            vyObs = data_dict[urls[c]]['zarr_store']['vy'][:, valid_idx[0][V], valid_idx[1][V]]\n            vxInv[data_dict[urls[c]]['ind_tot'], y_matches[i], x_matches[i]] = Inv_reg(vxObs, data_dict[urls[c]], fillvx)\n            vyInv[data_dict[urls[c]]['ind_tot'], y_matches[i], x_matches[i]] = Inv_reg(vyObs, data_dict[urls[c]], fillvy)\n            \n            i += 1\n            if i%100 == 0:\n                with open(\"Counter.txt\", \"w\") as text_file:\n                    text_file.write(f\"Counter: {i}\")\n            \n            if i%10000 == 0:\n                \n                print(f\"Saved at {i}\")\n                # Get the names of all the glaciers in the datacube \n                #glac_names = np.hstack(np.array([glacier.id.values[0] for glacier in gdf_list]))\n                #glac_names = '-'.join(glac_names)\n\n                # Gather the projection of the cube\n                #glac_proj = str(np.unique(np.hstack([data_dict[urls[i]]['proj_cube'] for i in range(len(urls))]))[0])\n\n                # Create a new dataset with vx and vy, using attributes from 'ds'\n                new_ds = xr.Dataset(\n                    {\n                        \"vx\": ([\"time\", \"y\", \"x\"], vxInv),\n                        \"vy\": ([\"time\", \"y\", \"x\"], vyInv),\n                    },\n                    coords={\n                        \"time\": ind_tot,\n                        \"x\": X_arr[x_min-1:x_max+1],\n                        \"y\": Y_arr[y_min-1:y_max+1],\n                    },\n                    attrs=data_dict[urls[0]]['zarr_store'].attrs,\n                ).chunk({'time': 1, 'x': 100, 'y': 100})\n\n                from dask.diagnostics import ProgressBar\n                write_job = new_ds.to_netcdf(f'Malaspina.nc', compute=False)\n                with ProgressBar():\n                    print(f\"Writing to {'Malaspina.nc'}\")\n                    write_job.compute()\n            progress_bar.update(1)  # Update the progress bar\n    \n    # Close the progress bar\n    progress_bar.close()\n\n    # Save the dataset\n    new_ds = xr.Dataset(\n        {\n            \"vx\": ([\"time\", \"y\", \"x\"], vxInv),\n            \"vy\": ([\"time\", \"y\", \"x\"], vyInv),\n        },\n        coords={\n            \"time\": ind_tot,\n            \"x\": X_arr[x_min-1:x_max+1],\n            \"y\": Y_arr[y_min-1:y_max+1],\n        },\n        attrs=data_dict[urls[0]]['zarr_store'].attrs,\n    ).chunk({'time': 1, 'x': 100, 'y': 100})\n\n    from dask.diagnostics import ProgressBar\n    write_job = new_ds.to_netcdf(f'Malaspina.nc', compute=False)\n    with ProgressBar():\n        print(f\"Writing to {'Malaspina.nc'}\")\n        write_job.compute()"
  },
  {
    "objectID": "obj_setup.html#defining-classes",
    "href": "obj_setup.html#defining-classes",
    "title": "04: Object setup",
    "section": "Defining classes",
    "text": "Defining classes\n\nSelecting data"
  },
  {
    "objectID": "obj_setup.html#creating-objects-of-glacier-glacier_centerline-glacier_point-classes",
    "href": "obj_setup.html#creating-objects-of-glacier-glacier_centerline-glacier_point-classes",
    "title": "04: Object setup",
    "section": "Creating objects of Glacier, Glacier_Centerline, Glacier_Point classes",
    "text": "Creating objects of Glacier, Glacier_Centerline, Glacier_Point classes\n\nCreate directly from widget\n\n\nCreate manually\n\n\nGlacier"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()"
  },
  {
    "objectID": "interactive.html",
    "href": "interactive.html",
    "title": "02: interactive_data_selection",
    "section": "",
    "text": "from ITS_LIVE_TOOL import obj_setup"
  },
  {
    "objectID": "interactive.html#when-just-one-glaciers-is-clicked-creating-a-single-object",
    "href": "interactive.html#when-just-one-glaciers-is-clicked-creating-a-single-object",
    "title": "02: interactive_data_selection",
    "section": "When just one glaciers is clicked / creating a single object:",
    "text": "When just one glaciers is clicked / creating a single object:"
  },
  {
    "objectID": "interactive.html#when-multiple-glaciers-are-selected-creating-multiple-objects-at-once",
    "href": "interactive.html#when-multiple-glaciers-are-selected-creating-multiple-objects-at-once",
    "title": "02: interactive_data_selection",
    "section": "When multiple glaciers are selected / creating multiple objects at once:",
    "text": "When multiple glaciers are selected / creating multiple objects at once:"
  },
  {
    "objectID": "data_inspection.html",
    "href": "data_inspection.html",
    "title": "05: Data inspection – slow-moving glaciers",
    "section": "",
    "text": "This is a notebook intended to highlight some of the questions that can arise, specifically when using ITS_LIVE to study slower moving glaciers. The notebook contains a few functions designed to filter the dataset based on different characteristics. They are designed to be modular and flexible. NOTE: these are not meant to be blindly implemented, rather to provide examples of possible solutions to outlier detection problems. Feel free to use and adjust them to best fit your use-case.\n\n\n\n\n\n\n\nITS_LIVE image pair data comes with an associated velocity error variable. It is calculated by : (insert equation from docs).\nOne way you may want to threshold your dataset is to use the v_error variable. This notebook provides an example of setting an acceptable threshold of v_error in relation to v, and excluding any observations where v_error exceeds this limit.\n\n\n\n\n\n\nthere is an issue w/ this fn, figure it out wednesday\n\n\n\n\n\n\n\n\n\n\n\n\n-add a citation for nyquist principle This method considers the expected ice movement of the selected point on the glacier based on long-term movement at this location and compares this value to the ground sampling distance of the imaging sensor to determine the minimum temporal baseline that would be needed to capture expected ice movement. To accomplish this, we take only the observations measured from image pairs where the temporal baseline is greater than 365 days. We then calculate the median velocity of all of these observations and use this as the expected displacement value. To calculate the minimum temporal baseline, we consider the expected displacement and the Nyquist Principle which states that sampling frequency must be at least twice the expected frequency of the data variability (rewrite) to avoid artifacts related to aliasing and other sampling distortions. The resulting dataset has a calculated minimum temporal baseline for each imaging sensor that contributes to the dataset. Due to the approximate nature of this approach and in order to minimize erroneously excluded observations we do not include a term related to the performance of the chip correlation algorithm, though this is another source of error that would increase the minimum temporal baseline over which ice displacement could be confidently detected.\n\n\n\n\nphysical basis. This approach is rooted in limitations of the satellite imaging sensors used and the long-term record of long temporal baseline velocity estimates which are typically characterized by less noise and higher confidence than shorter temporal baseline pairs. ### Cons\npotentially very overly-conservative. We are basing the expected ice movement on a long-term median velocity value. If shorter-term velocity velocity variability is significantly faster than this long-term, expected value than these observations may be incorrectly excluded.\nNeeds to be applied carefully if your analysis will include a spatial dimension. Put differently, this method in its current form would be inappropriate if it is separately calculated for each point included in analysis because it would create the situation where ice velocity estimates derived from different length image pairs are being compared.\n\n\n\n\n\n\n\n\n\n\n\nMuch simpler to apply at scale, across multiple points and glaciers\n\n\n\n\n\nLeast sensitive to the structure of the dataset\nNo explicit physical basis, this method is only as effective as the choice of threshold\n\n\n\n\n\nRecreating the above-plots for a point on a faster-moving glacier, we see a few interesting things.\nMalaspina glacier, Alaska"
  },
  {
    "objectID": "data_inspection.html#thresholding-using-v_error",
    "href": "data_inspection.html#thresholding-using-v_error",
    "title": "05: Data inspection – slow-moving glaciers",
    "section": "",
    "text": "ITS_LIVE image pair data comes with an associated velocity error variable. It is calculated by : (insert equation from docs).\nOne way you may want to threshold your dataset is to use the v_error variable. This notebook provides an example of setting an acceptable threshold of v_error in relation to v, and excluding any observations where v_error exceeds this limit.\n\n\n\n\n\n\nthere is an issue w/ this fn, figure it out wednesday"
  },
  {
    "objectID": "data_inspection.html#thresholding-via-min-temp-baseline-threshold",
    "href": "data_inspection.html#thresholding-via-min-temp-baseline-threshold",
    "title": "05: Data inspection – slow-moving glaciers",
    "section": "",
    "text": "-add a citation for nyquist principle This method considers the expected ice movement of the selected point on the glacier based on long-term movement at this location and compares this value to the ground sampling distance of the imaging sensor to determine the minimum temporal baseline that would be needed to capture expected ice movement. To accomplish this, we take only the observations measured from image pairs where the temporal baseline is greater than 365 days. We then calculate the median velocity of all of these observations and use this as the expected displacement value. To calculate the minimum temporal baseline, we consider the expected displacement and the Nyquist Principle which states that sampling frequency must be at least twice the expected frequency of the data variability (rewrite) to avoid artifacts related to aliasing and other sampling distortions. The resulting dataset has a calculated minimum temporal baseline for each imaging sensor that contributes to the dataset. Due to the approximate nature of this approach and in order to minimize erroneously excluded observations we do not include a term related to the performance of the chip correlation algorithm, though this is another source of error that would increase the minimum temporal baseline over which ice displacement could be confidently detected.\n\n\n\n\nphysical basis. This approach is rooted in limitations of the satellite imaging sensors used and the long-term record of long temporal baseline velocity estimates which are typically characterized by less noise and higher confidence than shorter temporal baseline pairs. ### Cons\npotentially very overly-conservative. We are basing the expected ice movement on a long-term median velocity value. If shorter-term velocity velocity variability is significantly faster than this long-term, expected value than these observations may be incorrectly excluded.\nNeeds to be applied carefully if your analysis will include a spatial dimension. Put differently, this method in its current form would be inappropriate if it is separately calculated for each point included in analysis because it would create the situation where ice velocity estimates derived from different length image pairs are being compared."
  },
  {
    "objectID": "data_inspection.html#blunt-temporal-baseline-minimum-for-all-sensors",
    "href": "data_inspection.html#blunt-temporal-baseline-minimum-for-all-sensors",
    "title": "05: Data inspection – slow-moving glaciers",
    "section": "",
    "text": "Much simpler to apply at scale, across multiple points and glaciers\n\n\n\n\n\nLeast sensitive to the structure of the dataset\nNo explicit physical basis, this method is only as effective as the choice of threshold"
  },
  {
    "objectID": "data_inspection.html#comparison-to-faster-flowing-glacier",
    "href": "data_inspection.html#comparison-to-faster-flowing-glacier",
    "title": "05: Data inspection – slow-moving glaciers",
    "section": "",
    "text": "Recreating the above-plots for a point on a faster-moving glacier, we see a few interesting things.\nMalaspina glacier, Alaska"
  }
]