{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ae12e1c-859d-4c9e-9009-56f6cf48f642",
   "metadata": {},
   "source": [
    "# 03: Velocity inversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a76102-ce85-425b-918d-bd106a8f668d",
   "metadata": {},
   "source": [
    "# NOTE: still need to organize this code more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17710cd2-1523-4e14-9ed4-5d2f4e71573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp invert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9a30185-f150-4d86-b79b-9cdfea641c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev \n",
    "from nbdev import nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b430b2e-3bac-4b7d-bd00-926b77daf2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import pyproj\n",
    "import matplotlib.path as path\n",
    "import s3fs\n",
    "import zarr\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import re\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.path as mplp\n",
    "import ipyleaflet as ipyl\n",
    "from ipyleaflet import WMSLayer\n",
    "import ipywidgets as ipyw\n",
    "import json\n",
    "import pandas as pd\n",
    "from ipyleaflet import Map, WMSLayer, basemaps\n",
    "from ipywidgets import HTML\n",
    "from owslib.wms import WebMapService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a731d171-1e95-4004-96c7-e8d021225caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ITS_LIVE_Analysis import setup, interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e0f6704b-3838-4dad-88b5-5ce3c6190f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "map = interactive.Widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2e8c3e89-7004-4bbe-a46f-5ff508e5504e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f1f422e5ad4e58849f88cb7e57f2c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Map(center=[0, 0], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_title', â€¦"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4b5ab3ae-c00a-439b-89f0-8ac95cb0e69a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m coords, gpdf, urls \u001b[38;5;241m=\u001b[39m interactive\u001b[38;5;241m.\u001b[39mreturn_clicked_info(\u001b[38;5;28mmap\u001b[39m)\n",
      "File \u001b[0;32m~/2023_fall/itslivetools/itslivetools/interactive.py:147\u001b[0m, in \u001b[0;36mreturn_clicked_info\u001b[0;34m(clicked_widget)\u001b[0m\n",
      "File \u001b[0;32m~/software/pkg/miniconda3/envs/itslivetools_env/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/software/pkg/miniconda3/envs/itslivetools_env/lib/python3.11/site-packages/pandas/core/reshape/concat.py:368\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat\u001b[39m(\n\u001b[1;32m    148\u001b[0m     objs: Iterable[NDFrame] \u001b[38;5;241m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    158\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    Concatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    1   3   4\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    369\u001b[0m         objs,\n\u001b[1;32m    370\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m    371\u001b[0m         ignore_index\u001b[38;5;241m=\u001b[39mignore_index,\n\u001b[1;32m    372\u001b[0m         join\u001b[38;5;241m=\u001b[39mjoin,\n\u001b[1;32m    373\u001b[0m         keys\u001b[38;5;241m=\u001b[39mkeys,\n\u001b[1;32m    374\u001b[0m         levels\u001b[38;5;241m=\u001b[39mlevels,\n\u001b[1;32m    375\u001b[0m         names\u001b[38;5;241m=\u001b[39mnames,\n\u001b[1;32m    376\u001b[0m         verify_integrity\u001b[38;5;241m=\u001b[39mverify_integrity,\n\u001b[1;32m    377\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    378\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    379\u001b[0m     )\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/software/pkg/miniconda3/envs/itslivetools_env/lib/python3.11/site-packages/pandas/core/reshape/concat.py:425\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    422\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "coords, gpdf, urls = interactive.return_clicked_info(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e14162e3-a723-401a-9838-2ace29cdc0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier1 = {'RGIID': gpdf.iloc[0]['RGIID'], \n",
    "           'coords': coords[0],\n",
    "            'url':urls[0],\n",
    "            'gdf':gpdf\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "54b632bc-696e-4e99-8cc5-35919524816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_utm_gdf = setup.point_to_gdf(glacier1['coords']).to_crs('EPSG:32643')\n",
    "coords_utm_gdf\n",
    "\n",
    "coords_utm_ls = [coords_utm_gdf['y'].iloc[0], coords_utm_gdf['x'].iloc[0]]\n",
    "coords_utm_ls\n",
    "\n",
    "urls = [glacier1['url']]\n",
    "\n",
    "gdf_list = [glacier1['gdf']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "19510c59-169a-4f41-996c-012c8d5ebc51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3://its-live-data/datacubes/v2/N30E070/ITS_LIVE_vel_EPSG32643_G0120_X650000_Y3950000.zarr']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "00c30f50-0fff-47a1-8448-f07ae4ab39ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://its-live-data/datacubes/v2/N30E070/ITS_LIVE_vel_EPSG32643_G0120_X650000_Y3950000.zarr\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "# Gather the points on the boundary of the glacier (should later be implemented as a GUI loading the points of a glacier's periphery)\n",
    "#boundary_points = pickle.load(open('boundary.p','rb'))\n",
    "\n",
    "# Modify the urls so they can be opened by zarr (replace 'http' by 's3' and delete '.s3.amazonaws.com')\n",
    "#urls = [url.replace('http','s3') for url in urls]\n",
    "urls = [re.sub(r'http', 's3', url) for url in urls]\n",
    "urls = [re.sub(r'\\.s3\\.amazonaws\\.com', '', url) for url in urls]\n",
    "print(urls[0])\n",
    "# Create storing arrays for the coordinates on-glacier\n",
    "X_valid = []\n",
    "Y_valid = []\n",
    "X_tot = []\n",
    "Y_tot = []\n",
    "\n",
    "# Create an empty directoryimport pickle to hold many variables all tied to the datacubes\n",
    "data_dict = {}\n",
    "\n",
    "# We iterate through the different datacubes so they can each have one instance of the variables below\n",
    "for url in urls:\n",
    "    zarr_store = None # To store the datacube's information and access its variables\n",
    "    dates = None # To store the dates at which the inversion will give values\n",
    "    A_m = None # 1st part of the design matrix\n",
    "    reg_mat_Inv = None # Regularization in time, 2nd part of the design matrix\n",
    "    mission = None # If you want to invert specifically for one mission in particular ('S1','L8','L9', etc...)\n",
    "    index_sort = None # Indices representing the sorted dates (from older to most recent)\n",
    "    inds_mission = None # Indices representing the sorted dates per mission chosen\n",
    "    ind_tot = None # Indices representing the indices of the pixels on the GOI\n",
    "    valid_idx = None # Easting and Northing values of the indices above\n",
    "    proj_cube = None # Projection of the datacube\n",
    "    \n",
    "    # Create a dictionary entry for the URL with the desired subsets\n",
    "    data_dict[url] = {\n",
    "        'zarr_store': zarr_store,\n",
    "        'dates': dates,\n",
    "        'A_m': A_m,\n",
    "        'reg_mat_Inv': reg_mat_Inv,\n",
    "        'mission': mission,\n",
    "        'index_sort': index_sort,\n",
    "        'inds_mission': inds_mission,\n",
    "        'dates': dates,\n",
    "        'ind_tot': ind_tot,\n",
    "        'valid_idx': valid_idx,\n",
    "        'proj_cube': proj_cube\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26469023-03f5-4721-ab40-fd4d23712bd2",
   "metadata": {},
   "source": [
    "## Datacubes Extent and Point Validity\n",
    "\n",
    "Designed to grab the extents of the datacubes, and determines which pixel belongs to the GOI. This way we do not lose time inverting for empty pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "14aef3fa-b7ee-4da6-942c-4762813fb538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "def get_extents(url, X_tot, Y_tot, X_valid, Y_valid, data_dict, mission, lamb, derivative, day_interval):\n",
    "\n",
    "    # Open the zarr files\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    store = zarr.open(s3fs.S3Map(url, s3=fs))\n",
    "   \n",
    "    # Update the dictionnary\n",
    "    data_dict[url]['zarr_store'] = store\n",
    "\n",
    "    # Get the cube's projection\n",
    "    proj_cube = store.attrs['projection']\n",
    "\n",
    "    # Load X and Y of the dataset\n",
    "    X = store['x'][:]\n",
    "    Y = store['y'][:]\n",
    "\n",
    "    # Store the arrays in the total list\n",
    "    X_tot.append(X)\n",
    "    Y_tot.append(Y)\n",
    "\n",
    "    # Load dimensions\n",
    "    shape_arr = store['v'].shape\n",
    "    \n",
    "    Xs, Ys = np.meshgrid(X, Y)\n",
    "    points = np.array((Xs.flatten(), Ys.flatten())).T\n",
    "\n",
    "    idx_valid = []\n",
    "    \n",
    "    for b in range(len(gdf_list)):\n",
    "        mpath = mplp.Path(list(gdf_list[b]['geometry'].to_crs(proj_cube).boundary.explode(index_parts = True).iloc[0].coords))\n",
    "        glacier_mask = mpath.contains_points(points).reshape(Xs.shape)\n",
    "        # Grab the indices of the points inside the glacier\n",
    "        idx_valid.append(np.array(np.where(glacier_mask==True)))\n",
    "        \n",
    "    idx_valid = np.hstack(idx_valid)\n",
    "    # Store the valid indices\n",
    "    data_dict[url]['valid_idx'] = idx_valid\n",
    "    \n",
    "    # Store the cube projection\n",
    "    data_dict[url]['proj_cube'] = proj_cube\n",
    "    \n",
    "    # Store the coordinates of the valid Xs and Ys\n",
    "    X_valid.append([Xs[idx_valid[0][i], idx_valid[1][i]] for i in range(len(idx_valid[0]))])\n",
    "    Y_valid.append([Ys[idx_valid[0][i], idx_valid[1][i]] for i in range(len(idx_valid[0]))])\n",
    "    \n",
    "    return X_tot, Y_tot, X_valid, Y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a38758-dfea-4b37-be7f-cb32f730f417",
   "metadata": {},
   "source": [
    "## Design Matrices\n",
    "\n",
    "This function creates 1 design matrix per cube. Knowing that each cube has different time stamps (different image pairs), but the possible dates for the image pairs are the same for every pixel of the datacube, we can pre-compute 1 design-matrix for each cube. We will simply mask-out the rows that represent time steps for which our point being inverted does not have any value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "61e098ad-fe77-4e93-81de-ee3889c4c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def design_matrices(url, mission, lamb, derivative, day_interval):\n",
    "\n",
    "    # If you passed 'mission' as an argument, it grabs the appropriate values\n",
    "    if mission:\n",
    "        # Get the indices of the mission\n",
    "        filt1 = np.where(data_dict[urls[url]]['zarr_store']['satellite_img1'][:] == mission)\n",
    "        filt2 = np.where(data_dict[urls[url]]['zarr_store']['satellite_img2'][:] == mission)\n",
    "        inds_mission = np.intersect1d(filt1[0],filt2[0])\n",
    "\n",
    "        # Grab only the indices corresponding to the missions\n",
    "        mid_dates = np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['mid_date'][:], dtype='timedelta64[D]')[inds_mission]\n",
    "        im1 = np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['acquisition_date_img1'][:], dtype='timedelta64[D]')[inds_mission]\n",
    "        im2 = np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['acquisition_date_img2'][:], dtype='timedelta64[D]')[inds_mission]\n",
    "    else:\n",
    "        # If 'None' was passed as a mission argument, we grab all the available data.\n",
    "        inds_mission = None\n",
    "        mid_dates = np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['mid_date'][:], dtype='timedelta64[D]')\n",
    "        im1 = np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['acquisition_date_img1'][:], dtype='timedelta64[D]')\n",
    "        im2 = np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['acquisition_date_img2'][:], dtype='timedelta64[D]')\n",
    "    \n",
    "    # Get some arrays\n",
    "    index_sort = np.argsort(np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['mid_date'][:], dtype='timedelta64[D]'))\n",
    "    mid_dates = mid_dates[index_sort]\n",
    "    im1 = im1[index_sort]\n",
    "    im2 = im2[index_sort]\n",
    "\n",
    "\n",
    "    # Check which im is the smallest (first image, it changes depending on ITS_LIVE's version)\n",
    "    if im2[0] < im1[0]:\n",
    "        temp = im1\n",
    "        im1 = im2\n",
    "        im2 = temp\n",
    "\n",
    "\n",
    "    # Create the date array with the new interval dates\n",
    "    dates_nonum = np.arange(mid_dates[0], mid_dates[-1], timedelta(days=day_interval)).astype(np.datetime64)\n",
    "\n",
    "    # Convert to numerical\n",
    "    dates = (dates_nonum - np.datetime64('1970-01-01T00:00:00Z'))/np.timedelta64(1, 's')\n",
    "    dt_start = (im1 - np.datetime64('1970-01-01T00:00:00Z'))/np.timedelta64(1, 's')\n",
    "    dt_end = (im2 - np.datetime64('1970-01-01T00:00:00Z'))/np.timedelta64(1, 's')\n",
    "\n",
    "    # --------------- DESIGN MATRICES --------------- \n",
    "\n",
    "    # Initialize matrix\n",
    "    A_m = np.zeros((mid_dates.shape[0],dates.shape[0]))\n",
    "\n",
    "    # We have to iterate through the satellite pairs that actually gave a measurement\n",
    "    for j in range(1, len(mid_dates)):\n",
    "    # current contents of your for loop\n",
    "\n",
    "        # Find the middate that is the closest to dt_start (supequal)\n",
    "        start = np.argmin(np.abs(dates-dt_start[j]))\n",
    "\n",
    "        # Find the middate that is closest to dt_end (infequal)\n",
    "        end = np.argmin(dt_end[j] - dates[dates <= dt_end[j]])\n",
    "\n",
    "        # Divide 1 by the amount of middates between d_start and d_end \n",
    "        if end == A_m.shape[1]-1: # If the mid_date is at the end of the array (acquisition im2 equals last mid_date)\n",
    "            A_m[j, start:] = 1/(1+A_m.shape[1]-start)\n",
    "        else: # If the measurement is in A's bounds temporally (we can have a satellite pair with the 2nd pair being outside of our mid_dates)\n",
    "            A_m[j, start:end+1] = 1/(1+end-start) # Attribute to each pixel in the timescale of the satellite pair, the 1/amount of pixel in the pairmid_dates.shape\n",
    "\n",
    "\n",
    "    # Initialize regularization matrix\n",
    "    if derivative == 1:\n",
    "        reg_mat_Inv = np.zeros((A_m.shape[1] -1, A_m.shape[1]))\n",
    "\n",
    "        for j in range(A_m.shape[1] -1):\n",
    "            reg_mat_Inv[j, j] = -lamb/day_interval\n",
    "            reg_mat_Inv[j, j+1] = lamb/day_interval\n",
    "\n",
    "    elif derivative == 2:\n",
    "        # Initialize 2nd derivative regularization matrix\n",
    "        reg_mat_Inv = np.zeros((A_m.shape[1] -1, A_m.shape[1]))\n",
    "\n",
    "        for j in range(A_m.shape[1] -2):\n",
    "            reg_mat_Inv[j, j] = lamb/(day_interval**2)\n",
    "            reg_mat_Inv[j, j+1] = -2*lamb/(day_interval**2)\n",
    "            reg_mat_Inv[j, j+2] = lamb/(day_interval**2)\n",
    "            \n",
    "    data_dict[urls[url]]['A_m'] = A_m\n",
    "    data_dict[urls[url]]['reg_mat_Inv'] = reg_mat_Inv\n",
    "    data_dict[urls[url]]['mission'] = mission\n",
    "    data_dict[urls[url]]['index_sort'] = index_sort\n",
    "    data_dict[urls[url]]['inds_mission'] = inds_mission\n",
    "    data_dict[urls[url]]['dates'] = dates_nonum\n",
    "            \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "181c5d1e-893a-4ee5-893a-d554d1d38582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def Inv_reg(vObs, data, fillvalue):\n",
    " \n",
    "    # Grab observed velocities\n",
    "    vObs = vObs[data['index_sort']]\n",
    "    \n",
    "    # Filter out the missions we don't want\n",
    "    if mission:\n",
    "        vObs = vObs[data['inds_mission']]  \n",
    "    \n",
    "    # Mask the NaNs so we don't compute the inversion for empty rows\n",
    "    mask = np.logical_not(np.equal(vObs, fillvalue))\n",
    "    \n",
    "    # Create a masked velocity vector\n",
    "    vObs_masked = vObs[mask]\n",
    "    \n",
    "    # Append regularization terms to dObs\n",
    "    vObs_masked= np.hstack((vObs_masked, np.zeros((data['reg_mat_Inv'].shape[0]))))\n",
    "     \n",
    "    # Assemble the design matrix\n",
    "    A_des = np.vstack((data['A_m'][mask], data['reg_mat_Inv']))\n",
    "    \n",
    "    # Invert the velocities\n",
    "    vInv = np.linalg.solve(A_des.T@A_des,A_des.T@vObs_masked)\n",
    "    \n",
    "    return vInv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6665b07-514d-4580-8266-1eb94915509f",
   "metadata": {},
   "source": [
    "# **Select Your Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2f02639f-e65f-4d6a-8e60-b41b272bc241",
   "metadata": {},
   "outputs": [],
   "source": [
    "mission = None # 'None' if you want all the data, 'S1' for Sentinel-1 only, 'L' for Landsat only, etc.. .\n",
    "lamb = 10 # Smoothing coefficient: the higher the value, the more the inversion favors a smooth output. BAD for surging glaciers, GOOD for non-surging glaciers\n",
    "derivative = 2 # Derivative degree for the inversion. Doesn't change much unless you have a specific reasong to choose 1 or 2 (1st or 2nd derivative)\n",
    "day_interval = 12 # Amount of days in between each inversion value. The higher, the faster the inversion. But you also lose in temporal resolution. 12 here because Sentinel-1 repeat-time is 12.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb76354-9e87-4af7-95ef-5c31600fc61d",
   "metadata": {},
   "source": [
    "## Get extents of each datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e027e81d-b161-4c6a-a6df-003ecf0c8006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                       id   CENLON  ZMAX   BGNDATE  ZMIN           RGIID  \\\n",
       " 0  RGI_SouthAsiaWest.6794  76.4047  8569  20010721  3385  RGI60-14.06794   \n",
       " \n",
       "    ASPECT   CENLAT  SLOPE  ZMED  ...  TERMTYPE  O2REGION  STATUS   ENDDATE  \\\n",
       " 0     296  35.7416   23.8  5393  ...         0         2       0  -9999999   \n",
       " \n",
       "    FORM SURGING         GLIMSID O1REGION             NAME  \\\n",
       " 0     0       3  G076405E35742N       14  Baltoro Glacier   \n",
       " \n",
       "                                             geometry  \n",
       " 0  MULTIPOLYGON (((76.54920 35.92760, 76.54830 35...  \n",
       " \n",
       " [1 rows x 24 columns]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7f63b000-c50d-44be-b47d-20127a413ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:28<00:00, 28.81s/it]\n"
     ]
    }
   ],
   "source": [
    "for url in tqdm(range(len(urls))):\n",
    "    X_tot, Y_tot, X_valid, Y_valid = get_extents(urls[url], X_tot, Y_tot, X_valid, Y_valid, data_dict, mission, lamb, derivative, day_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00161c05-6dda-4f53-a30e-5e6825dd5e3e",
   "metadata": {},
   "source": [
    "### For each datacube's valid indices (on the GOI), get the corresponding coordinates in a layer that gathers all the GOI parts spread accross the datacubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7ac0029b-87f8-4e57-9e2b-cb46fd1980dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Eastings and Northings arrays based on the Eastings and Northings of the datacubes\n",
    "X_arr = np.unique(np.hstack(X_tot))\n",
    "Y_arr = np.unique(np.hstack(Y_tot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5a61ee9b-2e1e-4c5b-9fb0-dba98e4cdd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop to the GOI (so we avoid over-filling our matrix with NaNs)\n",
    "x_min = np.where(np.min(np.hstack(X_valid)) == X_arr)[0][0]\n",
    "x_max = np.where(np.max(np.hstack(X_valid)) == X_arr)[0][0]\n",
    "y_min = np.where(np.min(np.hstack(Y_valid)) == Y_arr)[0][0]\n",
    "y_max = np.where(np.max(np.hstack(Y_valid)) == Y_arr)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1e716b04-e276-4909-a09a-c44c070a17d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# And now search the indices corresponding to the coordinates \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m x_matches \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([[np\u001b[38;5;241m.\u001b[39mwhere(i \u001b[38;5;241m==\u001b[39m X_arr[\u001b[38;5;28mmin\u001b[39m(x_min\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, x_max\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\u001b[38;5;28mmax\u001b[39m(x_min\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, x_max\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m row] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m X_valid])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m      3\u001b[0m y_matches \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([[np\u001b[38;5;241m.\u001b[39mwhere(i \u001b[38;5;241m==\u001b[39m Y_arr[\u001b[38;5;28mmin\u001b[39m(y_min\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, y_max\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\u001b[38;5;28mmax\u001b[39m(y_min\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, y_max\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m row] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m Y_valid])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "Cell \u001b[0;32mIn[69], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# And now search the indices corresponding to the coordinates \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m x_matches \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([[np\u001b[38;5;241m.\u001b[39mwhere(i \u001b[38;5;241m==\u001b[39m X_arr[\u001b[38;5;28mmin\u001b[39m(x_min\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, x_max\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\u001b[38;5;28mmax\u001b[39m(x_min\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, x_max\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m row] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m X_valid])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m      3\u001b[0m y_matches \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([[np\u001b[38;5;241m.\u001b[39mwhere(i \u001b[38;5;241m==\u001b[39m Y_arr[\u001b[38;5;28mmin\u001b[39m(y_min\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, y_max\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\u001b[38;5;28mmax\u001b[39m(y_min\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, y_max\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m row] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m Y_valid])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "Cell \u001b[0;32mIn[69], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# And now search the indices corresponding to the coordinates \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m x_matches \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([[np\u001b[38;5;241m.\u001b[39mwhere(i \u001b[38;5;241m==\u001b[39m X_arr[\u001b[38;5;28mmin\u001b[39m(x_min\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, x_max\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\u001b[38;5;28mmax\u001b[39m(x_min\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, x_max\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m row] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m X_valid])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m      3\u001b[0m y_matches \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([[np\u001b[38;5;241m.\u001b[39mwhere(i \u001b[38;5;241m==\u001b[39m Y_arr[\u001b[38;5;28mmin\u001b[39m(y_min\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, y_max\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\u001b[38;5;28mmax\u001b[39m(y_min\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, y_max\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m row] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m Y_valid])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "# And now search the indices corresponding to the coordinates \n",
    "x_matches = np.hstack([[np.where(i == X_arr[min(x_min-1, x_max+1):max(x_min-1, x_max+1)])[0][0] for i in row] for row in X_valid]).astype(int)\n",
    "y_matches = np.hstack([[np.where(i == Y_arr[min(y_min-1, y_max+1):max(y_min-1, y_max+1)])[0][0] for i in row] for row in Y_valid]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf3fc90-748a-4830-96e7-ec9ff10959c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array representing the glacier\n",
    "template = np.zeros((len(Y_arr[min(y_min-1, y_max+1):max(y_min-1, y_max+1)]), len( X_arr[min(x_min-1, x_max+1):max(x_min-1, x_max+1)])))\n",
    "template[y_matches, x_matches] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c2da20-2290-4162-8075-3953c6dfaa0c",
   "metadata": {},
   "source": [
    "Verify that the glacier looks as we would expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c541ac0e-d73e-4179-91d0-80d10f58ad37",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'template' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mpcolormesh(X_arr[x_min\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:x_max\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], Y_arr[y_min\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:y_max\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], template)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'template' is not defined"
     ]
    }
   ],
   "source": [
    "plt.pcolormesh(X_arr[x_min-1:x_max+1], Y_arr[y_min-1:y_max+1], template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d37c411-52bf-4577-b21e-7c9e0a5bfbd0",
   "metadata": {},
   "source": [
    "### Create the design matrices for each datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036bbdbf-752e-445d-98ed-193450892e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(urls))):\n",
    "    design_matrices(i, mission, lamb, derivative, day_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859874a9-480d-4619-b9ca-1be98287c88b",
   "metadata": {},
   "source": [
    "### Gather dates for each datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b9fe7c-a0bc-48bf-9128-c2950fd00234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total amount of temporal steps\n",
    "ind_tot = []\n",
    "for i in urls:\n",
    "    ind_tot.append(data_dict[i]['dates'])\n",
    "\n",
    "ind_tot = np.unique(np.hstack(ind_tot))\n",
    "\n",
    "for i in urls:\n",
    "    data_dict[i]['ind_tot'] = np.array([np.where(c == ind_tot)[0][0] for c in data_dict[urls[0]]['dates']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb88266-53e5-4d05-87cc-f3480e437c83",
   "metadata": {},
   "source": [
    "### Calculate the point-inversion for all the GOI pixels in the datacubes for Vx and Vy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b5e78-be88-4933-988e-dedb63c287ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "vxInv = np.zeros((len(ind_tot), template.shape[0], template.shape[1]))\n",
    "vyInv = np.zeros((vxInv.shape))\n",
    "\n",
    "# Define the total number of iterations\n",
    "total_iterations = len(y_matches)\n",
    "\n",
    "# Create a tqdm object with dynamic_ncols=False to suppress intermediate updates\n",
    "# Create a tqdm object with a larger mininterval to suppress intermediate updates\n",
    "progress_bar = tqdm(total=total_iterations, dynamic_ncols=False, mininterval=1.0)\n",
    "\n",
    "\n",
    "i = 0\n",
    "for c in range(len(urls)):\n",
    "    valid_idx = data_dict[urls[c]]['valid_idx']\n",
    "    fillvx = data_dict[urls[c]]['zarr_store']['vx'].fill_value\n",
    "    fillvy = data_dict[urls[c]]['zarr_store']['vy'].fill_value\n",
    "    \n",
    "    for V in range(len(valid_idx[0])):\n",
    "        vxObs = data_dict[urls[c]]['zarr_store']['vx'][:, valid_idx[0][V], valid_idx[1][V]]\n",
    "        vyObs = data_dict[urls[c]]['zarr_store']['vy'][:, valid_idx[0][V], valid_idx[1][V]]\n",
    "        vxInv[data_dict[urls[c]]['ind_tot'], y_matches[i], x_matches[i]] = Inv_reg(vxObs, data_dict[urls[c]], fillvx)\n",
    "        vyInv[data_dict[urls[c]]['ind_tot'], y_matches[i], x_matches[i]] = Inv_reg(vyObs, data_dict[urls[c]], fillvy)\n",
    "        \n",
    "        i += 1\n",
    "        progress_bar.update(1)  # Update the progress bar\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d9f99-06c5-41bf-83ed-e7c1f74bca42",
   "metadata": {},
   "source": [
    "### Save results to a netcdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb4017f-ac4c-4a0a-b021-e303521d04d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataset with vx and vy, using attributes from 'ds'\n",
    "new_ds = xr.Dataset(\n",
    "    {\n",
    "        \"vx\": ([\"time\", \"y\", \"x\"], vxInv),\n",
    "        \"vy\": ([\"time\", \"y\", \"x\"], vyInv),\n",
    "    },\n",
    "    coords={\n",
    "        \"time\": ind_tot,\n",
    "        \"x\": X_arr[x_min-1:x_max+1],\n",
    "        \"y\": Y_arr[y_min-1:y_max+1],\n",
    "    },\n",
    "    attrs=data_dict[urls[0]]['zarr_store'].attrs,\n",
    ").chunk({'time': 1, 'x': 100, 'y': 100})\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "write_job = new_ds.to_netcdf('Inverted_Cube.nc', compute=False)\n",
    "with ProgressBar():\n",
    "    print(f\"Writing to {'Inverted_Cube.nc'}\")\n",
    "    write_job.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67247c1-f878-4e2b-9d3c-ffd7869668dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = 100\n",
    "c = 0\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "test = data_dict[urls[c]]['zarr_store']['vx'][:, valid_idx[0][V], valid_idx[1][V]].astype(np.float32)\n",
    "test[test<-25000] = np.nan\n",
    "plt.scatter(data_dict[urls[c]]['zarr_store']['mid_date'][:], test, label='Observed')\n",
    "plt.plot(ind_tot, vxInv[:, y_matches[500], x_matches[500]], color = 'orange',label='Inverted')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3dfc55-8af6-44b3-a0bd-044fa5f945cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
