{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ae12e1c-859d-4c9e-9009-56f6cf48f642",
   "metadata": {},
   "source": [
    "# 03: Velocity inversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a76102-ce85-425b-918d-bd106a8f668d",
   "metadata": {},
   "source": [
    "# NOTE: still need to organize this code more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17710cd2-1523-4e14-9ed4-5d2f4e71573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp invert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9a30185-f150-4d86-b79b-9cdfea641c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev \n",
    "from nbdev import nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b430b2e-3bac-4b7d-bd00-926b77daf2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import pyproj\n",
    "import matplotlib.path as path\n",
    "import s3fs\n",
    "import zarr\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import re\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.path as mplp\n",
    "import ipyleaflet as ipyl\n",
    "from ipyleaflet import WMSLayer\n",
    "import ipywidgets as ipyw\n",
    "import json\n",
    "import pandas as pd\n",
    "from ipyleaflet import Map, WMSLayer, basemaps, GeoData\n",
    "from ipywidgets import HTML\n",
    "from owslib.wms import WebMapService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a731d171-1e95-4004-96c7-e8d021225caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.1.1'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var is_dev = py_version.indexOf(\"+\") !== -1 || py_version.indexOf(\"-\") !== -1;\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n  var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'jspanel': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/jspanel', 'jspanel-modal': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal', 'jspanel-tooltip': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip', 'jspanel-hint': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint', 'jspanel-layout': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout', 'jspanel-contextmenu': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu', 'jspanel-dock': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock', 'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@7.2.3/dist/gridstack-all', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'jspanel': {'exports': 'jsPanel'}, 'gridstack': {'exports': 'GridStack'}}});\n      require([\"jspanel\"], function(jsPanel) {\n\twindow.jsPanel = jsPanel\n\ton_load()\n      })\n      require([\"jspanel-modal\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-tooltip\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-hint\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-layout\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-contextmenu\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-dock\"], function() {\n\ton_load()\n      })\n      require([\"gridstack\"], function(GridStack) {\n\twindow.GridStack = GridStack\n\ton_load()\n      })\n      require([\"notyf\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 9;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window['jsPanel'] !== undefined) && (!(window['jsPanel'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/jspanel.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/gridstack/gridstack@7.2.3/dist/gridstack-all.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.1.1.min.js\", \"https://cdn.holoviz.org/panel/1.2.3/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      Bokeh = root.Bokeh;\n      bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      if (!reloading && (!bokeh_loaded || is_dev)) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| export\n",
    "from ITS_LIVE_TOOL import setup, interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d35cdb3a-a8ae-46c2-873d-304f389f88cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0f6704b-3838-4dad-88b5-5ce3c6190f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "data_map = interactive.Widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e8c3e89-7004-4bbe-a46f-5ff508e5504e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12cb29740ef14fbe8051906d7f8226f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Map(center=[0, 0], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_title', …"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_map.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b5ab3ae-c00a-439b-89f0-8ac95cb0e69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#|export\n",
    "try: \n",
    "    coords, gpdf, urls = interactive.return_clicked_info(data_map)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce040488",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### EMMA IN NOTEBOOK 2 ADD 'GeoData' to the import line: from ipyleaflet import Map, WMSLayer, basemaps, GeoData\n",
    "\n",
    "t = []\n",
    "wms_url = \"https://glims.org/geoserver/ows?SERVICE=WMS&\"\n",
    "wms_layer = WMSLayer(\n",
    "    url=wms_url,\n",
    "    layers='GLIMS:RGI',\n",
    "    transparent=True,\n",
    "    format='image/png'\n",
    ")\n",
    "\n",
    "\n",
    "# Create a Pandas DataFrame with X and Y coordinates\n",
    "#data = pd.DataFrame({'X': list(fx_map[::50]), 'Y': list(fy_map[::50])})\n",
    "\n",
    "# Map and label widgets\n",
    "map = ipyl.Map(basemap=basemaps.Esri.WorldImagery, center=(0, 0), zoom=2)\n",
    "label = ipyw.Label(layout=ipyw.Layout(width=\"100%\"))\n",
    "\n",
    "# Create a list to store clicked URLs\n",
    "urls = []\n",
    "\n",
    "# geojson layer with hover handler\n",
    "with open(\"catalog_v02.json\") as f:\n",
    "    geojson_data = json.load(f)\n",
    "\n",
    "for feature in geojson_data[\"features\"]:\n",
    "    feature[\"properties\"][\"style\"] = {\n",
    "        \"color\": \"grey\",\n",
    "        \"weight\": 1,\n",
    "        \"fillColor\": \"grey\",\n",
    "        \"fillOpacity\": 0.5,\n",
    "    }\n",
    "\n",
    "geojson_layer = ipyl.GeoJSON(data=geojson_data, hover_style={\"fillColor\": \"red\"}, selected_style = {\"fillColor\": \"orange\"})\n",
    "\n",
    "def hover_handler(event=None, feature=None, id=None, properties=None):\n",
    "    label.value = properties[\"zarr_url\"]\n",
    "\n",
    "def json_handler(event=None, feature=None, id=None, properties=None):    \n",
    "    zarr_url = properties.get(\"zarr_url\", \"N/A\")\n",
    "    urls.append(zarr_url)\n",
    "    print(f\"Clicked URL: {zarr_url}\")\n",
    "    print(\"All Clicked URLs:\", urls)\n",
    "    '''\n",
    "    # Plot in orange the polygon clicked on\n",
    "    json_shape = [{'type': properties['geometry_epsg']['type'], 'coordinates': properties['geometry_epsg']['coordinates']}]\n",
    "    geom = [shape(i) for i in json_shape]\n",
    "    json_shape = gpd.GeoDataFrame({'geometry':geom})  \n",
    "    json_shape = GeoData(geo_dataframe = json_shape,\n",
    "                style={'color': 'black', 'fillColor': '#3366cc', 'opacity':0.05, 'weight':1.9, 'dashArray':'2', 'fillOpacity':0.6},\n",
    "                hover_style={'fillColor': 'blue' , 'fillOpacity': 0.2},\n",
    "                name = 'Cube')\n",
    "\n",
    "    map.add_layer(json_shape)\n",
    "    '''\n",
    "\n",
    "    \n",
    "    \n",
    "wms = WebMapService(wms_url)\n",
    "\n",
    "# Initialize a list to store the geopandas dataframes\n",
    "gdf_list = []\n",
    "\n",
    "# Create a function to handle click events\n",
    "def click_handler(properties=None, **kwargs):\n",
    "\n",
    "\n",
    "    if kwargs.get('type') == 'contextmenu':\n",
    "        latlon = kwargs.get('coordinates')\n",
    "        lat, lon = latlon[0], latlon[1]\n",
    "        print(f\"Clicked at (Lat: {lat}, Lon: {lon})\")\n",
    "        \n",
    "        # Arrange the coordinates\n",
    "        \n",
    "        response = wms.getfeatureinfo(\n",
    "            layers=['GLIMS:RGI'],\n",
    "            srs='EPSG:4326',\n",
    "            bbox=(lon-0.001,lat-0.001,lon+0.001,lat+0.001),\n",
    "            size=(1,1),\n",
    "            format='image/jpeg',\n",
    "            query_layers=['GLIMS:RGI'],\n",
    "            info_format=\"application/json\",\n",
    "            xy=(0,0))\n",
    "        df = gpd.read_file(response)\n",
    "        gdf_list.append(df)  \n",
    "        try:\n",
    "            print(f\"You have selected the glacier {df['NAME'].values[0]}, ID: {df['id'].values[0]} \")\n",
    "        except:\n",
    "            print(f\"This glacier is not recognized by the RGI (maybe an ice-shelf ?) -> Choose another one\")\n",
    "        geo_data = GeoData(geo_dataframe = df,\n",
    "                   style={'color': 'black', 'fillColor': '#3366cc', 'opacity':0.05, 'weight':1.9, 'dashArray':'2', 'fillOpacity':0.6},\n",
    "                   hover_style={'fillColor': 'blue' , 'fillOpacity': 0.2},\n",
    "                   name = 'Glacier')\n",
    "\n",
    "        gdf_list.append(df) \n",
    "        map.add_layer(geo_data)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "geojson_layer.on_hover(hover_handler)\n",
    "geojson_layer.on_click(json_handler)\n",
    "\n",
    "\n",
    "map.add(geojson_layer)\n",
    "\n",
    "# Add points from the DataFrame to the map\n",
    "#for index, row in data.iterrows():\n",
    "#    marker = ipyl.Marker(locatfairbanks ash situationion=(row['Y'], row['X']))\n",
    "#    map.add_layer(marker)\n",
    "\n",
    "# Enable zoom when scrolling with the mouse\n",
    "map.scroll_wheel_zoom = True\n",
    "ipyw.VBox([map, label])\n",
    "\n",
    "\n",
    "# Add the WMS layer to the map\n",
    "map.add_layer(wms_layer)\n",
    "\n",
    "# Add the click event handler to the map\n",
    "map.on_interaction(click_handler)\n",
    "\n",
    "map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c656ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EMMA THIS IS THE CELL TO MODIFY\n",
    "\n",
    "# Get indices of unique geopandas dataframes in case you selected the same glacier twice\n",
    "unique_values, unique_indices = np.unique(np.array([i['id'] for i in gdf_list]), return_index=True)\n",
    "\n",
    "# Iterate through the unique indices to recreate the list of geopandas dataframes unique only\n",
    "gdf_list = [gdf_list[i] for i in unique_indices]\n",
    "\n",
    "print(f\"You have {len(gdf_list)} glaciers selected\")\n",
    "\n",
    "if len(urls) == 0:\n",
    "    print('Select a datacube to fetch the data !!')\n",
    "\n",
    "# Print the first dataframe to show the structure of it\n",
    "gdf_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80cc97ff-7615-4a12-8553-4de04e9eb42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[29.276159989936716, 83.78609332964868]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0909783-b0ca-4cdb-8582-d0f83fbc7bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>CENLON</th>\n",
       "      <th>ZMAX</th>\n",
       "      <th>BGNDATE</th>\n",
       "      <th>ZMIN</th>\n",
       "      <th>RGIID</th>\n",
       "      <th>ASPECT</th>\n",
       "      <th>CENLAT</th>\n",
       "      <th>SLOPE</th>\n",
       "      <th>ZMED</th>\n",
       "      <th>...</th>\n",
       "      <th>TERMTYPE</th>\n",
       "      <th>O2REGION</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>ENDDATE</th>\n",
       "      <th>FORM</th>\n",
       "      <th>SURGING</th>\n",
       "      <th>GLIMSID</th>\n",
       "      <th>O1REGION</th>\n",
       "      <th>NAME</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RGI_CentralAsia.26883</td>\n",
       "      <td>83.787</td>\n",
       "      <td>6074</td>\n",
       "      <td>20090130</td>\n",
       "      <td>5665</td>\n",
       "      <td>RGI60-13.26885</td>\n",
       "      <td>324</td>\n",
       "      <td>29.276</td>\n",
       "      <td>16.7</td>\n",
       "      <td>5843</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999999</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>G083787E29276N</td>\n",
       "      <td>13</td>\n",
       "      <td>CN5O257C0025</td>\n",
       "      <td>MULTIPOLYGON (((83.78740 29.27100, 83.78710 29...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id  CENLON  ZMAX   BGNDATE  ZMIN           RGIID  \\\n",
       "0  RGI_CentralAsia.26883  83.787  6074  20090130  5665  RGI60-13.26885   \n",
       "\n",
       "   ASPECT  CENLAT  SLOPE  ZMED  ...  TERMTYPE  O2REGION  STATUS   ENDDATE  \\\n",
       "0     324  29.276   16.7  5843  ...         0         8       0  -9999999   \n",
       "\n",
       "   FORM SURGING         GLIMSID O1REGION          NAME  \\\n",
       "0     0       9  G083787E29276N       13  CN5O257C0025   \n",
       "\n",
       "                                            geometry  \n",
       "0  MULTIPOLYGON (((83.78740 29.27100, 83.78710 29...  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpdf[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "62405ba5-27ab-4e5e-a872-da58b5da4a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://its-live-data.s3.amazonaws.com/datacubes/v2/N20E080/ITS_LIVE_vel_EPSG32644_G0120_X750000_Y3250000.zarr']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80c4ce09-09b8-4776-8b6c-f537cbe60cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "try: \n",
    "    gpdf_ls = gpdf[0]\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6abacac2-9325-4c5e-914b-0614a818463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_input_dict(coords, gpdf, urls):\n",
    "    \n",
    "    mod_urls = [re.sub(r'http', 's3', url) for url in urls]\n",
    "    mod_urls = [re.sub(r'\\.s3\\.amazonaws\\.com', '', url) for url in mod_urls]\n",
    "    \n",
    "    d = {'coords': coords,\n",
    "     'gpdf': gpdf,\n",
    "     'urls': mod_urls\n",
    "        }\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39ea8152-7b9b-4218-9262-88ca9b7a8b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " no input data specified, have you made a map selection?\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "try:\n",
    "    d = make_input_dict(coords, gpdf, URLs)\n",
    "    urls = d['urls']\n",
    "except:\n",
    "    print(' no input data specified, have you made a map selection?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67bb65de-9fca-4c1b-a24a-e3c85e879f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_data_dict(urls):\n",
    "    # Modify the urls so they can be opened by zarr (replace 'http' by 's3' and delete '.s3.amazonaws.com')\n",
    "    mod_urls = [re.sub(r'http', 's3', url) for url in urls]\n",
    "    mod_urls = [re.sub(r'\\.s3\\.amazonaws\\.com', '', url) for url in urls]\n",
    "    \n",
    "    # Create storing arrays for the coordinates on-glacier\n",
    "    X_valid = []\n",
    "    Y_valid = []\n",
    "    X_tot = []\n",
    "    Y_tot = []\n",
    "    \n",
    "    # Create an empty directoryimport pickle to hold many variables all tied to the datacubes\n",
    "    data_dict = {}\n",
    "    \n",
    "\n",
    "    # We iterate through the different datacubes so they can each have one instance of the variables below\n",
    "    for mod_urls in urls:\n",
    "        zarr_store = None # To store the datacube's information and access its variables\n",
    "        dates = None # To store the dates at which the inversion will give values\n",
    "        A_m = None # 1st part of the design matrix\n",
    "        reg_mat_Inv = None # Regularization in time, 2nd part of the design matrix\n",
    "        mission = None # If you want to invert specifically for one mission in particular ('S1','L8','L9', etc...)\n",
    "        index_sort = None # Indices representing the sorted dates (from older to most recent)\n",
    "        inds_mission = None # Indices representing the sorted dates per mission chosen\n",
    "        ind_tot = None # Indices representing the indices of the pixels on the GOI\n",
    "        valid_idx = None # Easting and Northing values of the indices above\n",
    "        proj_cube = None # Projection of the datacube\n",
    "        mask_dates = None # Mask that filters out dates outside of desired date range\n",
    "        \n",
    "        # Create a dictionary entry for the URL with the desired subsets\n",
    "        data_dict[mod_urls] = {\n",
    "            'zarr_store': zarr_store,\n",
    "            'dates_noinv': dates,\n",
    "            'A_m': A_m,\n",
    "            'reg_mat_Inv': reg_mat_Inv,\n",
    "            'mission': mission,\n",
    "            'index_sort': index_sort,\n",
    "            'inds_mission': inds_mission,\n",
    "            'dates': dates,\n",
    "            'ind_tot': ind_tot,\n",
    "            'valid_idx': valid_idx,\n",
    "            'proj_cube': proj_cube,\n",
    "            'mask_dates': mask_dates\n",
    "        }\n",
    "        \n",
    "    return data_dict, X_valid, Y_valid, X_tot, Y_tot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "243a75a0-095d-4da9-9c34-19c189ddf7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def get_extents(url, gpdf, X_tot, Y_tot, X_valid, Y_valid, data_dict):# mission, lamb, derivative, day_interval):\n",
    "    \n",
    "    #url = input_data_dict['urls'].iloc[0]\n",
    "    \n",
    "    # Open the zarr files\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    store = zarr.open(s3fs.S3Map(url, s3=fs))\n",
    "   \n",
    "    # Update the dictionnary\n",
    "    data_dict[url]['zarr_store'] = store\n",
    "\n",
    "    # Get the cube's projection\n",
    "    proj_cube = store.attrs['projection']\n",
    "\n",
    "    # Load X and Y of the dataset\n",
    "    X = store['x'][:]\n",
    "    Y = store['y'][:]\n",
    "\n",
    "    # Store the arrays in the total list\n",
    "    X_tot.append(X)\n",
    "    Y_tot.append(Y)\n",
    "\n",
    "    # Load dimensions\n",
    "    shape_arr = store['v'].shape\n",
    "    \n",
    "    Xs, Ys = np.meshgrid(X, Y)\n",
    "    points = np.array((Xs.flatten(), Ys.flatten())).T\n",
    "\n",
    "    idx_valid = []\n",
    "    \n",
    "    for b in range(len(gpdf)):\n",
    "        mpath = mplp.Path(list(gpdf[b]['geometry'].to_crs(np.int(proj_cube)).boundary.explode(index_parts = True).iloc[0].coords))\n",
    "        glacier_mask = mpath.contains_points(points).reshape(Xs.shape)\n",
    "        # Grab the indices of the points inside the glacier\n",
    "        idx_valid.append(np.array(np.where(glacier_mask==True)))\n",
    "        \n",
    "    idx_valid = np.hstack(idx_valid)\n",
    "    # Store the valid indices\n",
    "    data_dict[url]['valid_idx'] = idx_valid\n",
    "    \n",
    "    # Store the cube projection\n",
    "    data_dict[url]['proj_cube'] = proj_cube\n",
    "    \n",
    "    # Store the coordinates of the valid Xs and Ys\n",
    "    X_valid.append([Xs[idx_valid[0][i], idx_valid[1][i]] for i in range(len(idx_valid[0]))])\n",
    "    Y_valid.append([Ys[idx_valid[0][i], idx_valid[1][i]] for i in range(len(idx_valid[0]))])\n",
    "    \n",
    "    return X_tot, Y_tot, X_valid, Y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73cc7f64-3925-46c3-824d-df9bcae5adcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def design_matrices(url, mission, lamb, derivative, day_interval, sdate, edate):\n",
    "\n",
    "    # If you passed 'mission' as an argument, it grabs the appropriate values\n",
    "    if mission:\n",
    "        # Get the indices of the mission\n",
    "        filt1 = np.where(data_dict[urls[url]]['zarr_store']['satellite_img1'][:] == mission)\n",
    "        filt2 = np.where(data_dict[urls[url]]['zarr_store']['satellite_img2'][:] == mission)\n",
    "        inds_mission = np.intersect1d(filt1[0],filt2[0])\n",
    "\n",
    "        # Grab only the indices corresponding to the missions\n",
    "        mid_dates = np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['mid_date'][:], dtype='timedelta64[D]')[inds_mission]\n",
    "        im1 = np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['acquisition_date_img1'][:], dtype='timedelta64[D]')[inds_mission]\n",
    "        im2 = np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['acquisition_date_img2'][:], dtype='timedelta64[D]')[inds_mission]\n",
    "    else:\n",
    "        # If 'None' was passed as a mission argument, we grab all the available data.\n",
    "        inds_mission = None\n",
    "        mid_dates = np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['mid_date'][:], dtype='timedelta64[D]')\n",
    "        im1 = np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['acquisition_date_img1'][:], dtype='timedelta64[D]')\n",
    "        im2 = np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['acquisition_date_img2'][:], dtype='timedelta64[D]')\n",
    "    \n",
    "    # Get some arrays\n",
    "    index_sort = np.argsort(np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['mid_date'][:], dtype='timedelta64[D]'))\n",
    "    mid_dates = mid_dates[index_sort]\n",
    "    im1 = im1[index_sort]\n",
    "    im2 = im2[index_sort]\n",
    "\n",
    "    # If sdate is later than the first available date, we find its corresponding index\n",
    "    try:\n",
    "        sdate_ind = np.where(mid_dates >= sdate)[0][0]\n",
    "    except:\n",
    "        sdate_ind = 0\n",
    "    \n",
    "    # If edate is sooner than the last available date, we find its corresponding index\n",
    "    try:\n",
    "        edate_ind = np.where(mid_dates > edate)[0][0]\n",
    "    except:\n",
    "        edate_ind = None\n",
    "    \n",
    "    # Create a False/True mask where True if the date is in the desired range\n",
    "    mask_dates = np.full(mid_dates.shape, False)\n",
    "    mask_dates[sdate_ind:edate_ind] = True\n",
    "\n",
    "    # Keep only the values within the desired range\n",
    "    mid_dates = mid_dates[mask_dates]\n",
    "    im1 = im1[mask_dates]\n",
    "    im2 = im2[mask_dates]\n",
    "\n",
    "    # Check which im is the smallest (first image, it changes depending on ITS_LIVE's version)\n",
    "    if im2[0] < im1[0]:\n",
    "        temp = im1\n",
    "        im1 = im2\n",
    "        im2 = temp\n",
    "\n",
    "    # Create the date array with the new interval dates\n",
    "    dates_nonum = np.arange(mid_dates[0], mid_dates[-1], timedelta(days=day_interval)).astype(np.datetime64)\n",
    "\n",
    "    # Convert to numerical\n",
    "    dates = (dates_nonum - np.datetime64('1970-01-01T00:00:00Z'))/np.timedelta64(1, 's')\n",
    "    dt_start = (im1 - np.datetime64('1970-01-01T00:00:00Z'))/np.timedelta64(1, 's')\n",
    "    dt_end = (im2 - np.datetime64('1970-01-01T00:00:00Z'))/np.timedelta64(1, 's')\n",
    "\n",
    "    # --------------- DESIGN MATRICES --------------- \n",
    "\n",
    "    # Initialize matrix\n",
    "    A_m = np.zeros((mid_dates.shape[0],dates.shape[0]))\n",
    "\n",
    "    # We have to iterate through the satellite pairs that actually gave a measurement\n",
    "    for j in range(1, len(mid_dates)):\n",
    "    # current contents of your for loop\n",
    "\n",
    "        # Find the middate that is the closest to dt_start (supequal)\n",
    "        start = np.argmin(np.abs(dates-dt_start[j]))\n",
    "\n",
    "        # Find the middate that is closest to dt_end (infequal)\n",
    "        end = np.argmin(dt_end[j] - dates[dates <= dt_end[j]])\n",
    "\n",
    "        # Divide 1 by the amount of middates between d_start and d_end \n",
    "        if end == A_m.shape[1]-1: # If the mid_date is at the end of the array (acquisition im2 equals last mid_date)\n",
    "            A_m[j, start:] = 1/(1+A_m.shape[1]-start)\n",
    "        else: # If the measurement is in A's bounds temporally (we can have a satellite pair with the 2nd pair being outside of our mid_dates)\n",
    "            A_m[j, start:end+1] = 1/(1+end-start) # Attribute to each pixel in the timescale of the satellite pair, the 1/amount of pixel in the pairmid_dates.shape\n",
    "\n",
    "\n",
    "    # Initialize regularization matrix\n",
    "    if derivative == 1:\n",
    "        reg_mat_Inv = np.zeros((A_m.shape[1] -1, A_m.shape[1]))\n",
    "\n",
    "        for j in range(A_m.shape[1] -1):\n",
    "            reg_mat_Inv[j, j] = -lamb/day_interval\n",
    "            reg_mat_Inv[j, j+1] = lamb/day_interval\n",
    "\n",
    "    elif derivative == 2:\n",
    "        # Initialize 2nd derivative regularization matrix\n",
    "        reg_mat_Inv = np.zeros((A_m.shape[1] -1, A_m.shape[1]))\n",
    "\n",
    "        for j in range(A_m.shape[1] -2):\n",
    "            reg_mat_Inv[j, j] = lamb/(day_interval**2)\n",
    "            reg_mat_Inv[j, j+1] = -2*lamb/(day_interval**2)\n",
    "            reg_mat_Inv[j, j+2] = lamb/(day_interval**2)\n",
    "            \n",
    "    data_dict[urls[url]]['A_m'] = A_m\n",
    "    data_dict[urls[url]]['reg_mat_Inv'] = reg_mat_Inv\n",
    "    data_dict[urls[url]]['mission'] = mission\n",
    "    data_dict[urls[url]]['index_sort'] = index_sort\n",
    "    data_dict[urls[url]]['inds_mission'] = inds_mission\n",
    "    data_dict[urls[url]]['dates'] = dates_nonum\n",
    "    data_dict[urls[url]]['dates_noinv'] = mid_dates\n",
    "    data_dict[urls[url]]['mask_dates']= mask_dates\n",
    "            \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "365519af-4ce6-4779-90ba-dee7c3eaf864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def Inv_reg(vObs, data, fillvalue):\n",
    "    \n",
    "    # Grab observed velocities\n",
    "    vObs = vObs[data['index_sort']]\n",
    "    vObs = vObs[data['mask_dates']]\n",
    "    \n",
    "    # Filter out the missions we don't want\n",
    "    if mission:\n",
    "        vObs = vObs[data['inds_mission']]  \n",
    "    \n",
    "    # Mask the NaNs so we don't compute the inversion for empty rows\n",
    "    mask = np.logical_not(np.equal(vObs, fillvalue))\n",
    "    \n",
    "    # Create a masked velocity vector\n",
    "    vObs_masked = vObs[mask]\n",
    "    \n",
    "    # Append regularization terms to dObs\n",
    "    vObs_masked= np.hstack((vObs_masked, np.zeros((data['reg_mat_Inv'].shape[0]))))\n",
    "     \n",
    "    # Assemble the design matrix\n",
    "    A_des = np.vstack((data['A_m'][mask], data['reg_mat_Inv']))\n",
    "    \n",
    "    # Invert the velocities\n",
    "    vInv = np.linalg.solve(A_des.T@A_des,A_des.T@vObs_masked)\n",
    "    #vInv = scipy.linalg.solve(A_des.T@A_des,A_des.T@vObs_masked, lower = False, check_finite=False, assume_a='gen')\n",
    "    \n",
    "    return vInv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee92697d-4879-4158-ad7d-402ca6879bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "#| export \n",
    "try:\n",
    "    data_dict, X_valid, Y_valid, X_tot, Y_tot = create_data_dict(urls)\n",
    "\n",
    "    for url in tqdm(range(len(urls))):\n",
    "        X_tot, Y_tot, X_valid, Y_valid = get_extents(urls[url], gpdf_ls, X_tot, Y_tot, X_valid, Y_valid, data_dict)\n",
    "\n",
    "    mission = None # 'None' if you want all the data, 'S1' for Sentinel-1 only, 'L' for Landsat only, etc.. .\n",
    "    lamb = 10 # Smoothing coefficient: the higher the value, the more the inversion favors a smooth output. BAD for surging glaciers, GOOD for non-surging glaciers\n",
    "    derivative = 2 # Derivative degree for the inversion. Doesn't change much unless you have a specific reasong to choose 1 or 2 (1st or 2nd derivative)\n",
    "    day_interval = 12 # Amount of days in between each inversion value. The higher, the faster the inversion. But you also lose in temporal resolution. 12 here because Sentinel-1 repeat-time is 12.\n",
    "    sdate = None # Start date, format 'YYYY-MM-DD' OR None if you want to grab the entire timeseries available\n",
    "    edate = None # End date, format 'YYYY-MM-DD' OR None if you want to grab the entire timeseries available\n",
    "    GPU = True # None if you want to run it on CPU\n",
    "\n",
    "    # Create Eastings and Northings arrays based on the Eastings and Northings of the datacubes\n",
    "    X_arr = np.unique(np.hstack(X_tot))\n",
    "    Y_arr = np.unique(np.hstack(Y_tot))\n",
    "    \n",
    "    # Crop to the GOI (so we avoid over-filling our matrix with NaNs)\n",
    "    x_min = np.where(np.min(np.hstack(X_valid)) == X_arr)[0][0]\n",
    "    x_max = np.where(np.max(np.hstack(X_valid)) == X_arr)[0][0]\n",
    "    y_min = np.where(np.min(np.hstack(Y_valid)) == Y_arr)[0][0]\n",
    "    y_max = np.where(np.max(np.hstack(Y_valid)) == Y_arr)[0][0]\n",
    "    \n",
    "    \n",
    "    # And now search the indices corresponding to the coordinates \n",
    "    x_matches = np.hstack([[np.where(i == X_arr[min(x_min-1, x_max+1):max(x_min-1, x_max+1)])[0][0] for i in row] for row in X_valid]).astype(int)\n",
    "    y_matches = np.hstack([[np.where(i == Y_arr[min(y_min-1, y_max+1):max(y_min-1, y_max+1)])[0][0] for i in row] for row in Y_valid]).astype(int)\n",
    "    \n",
    "    # Create an array representing the glacier\n",
    "    template = np.zeros((len(Y_arr[min(y_min-1, y_max+1):max(y_min-1, y_max+1)]), len( X_arr[min(x_min-1, x_max+1):max(x_min-1, x_max+1)])))\n",
    "    template[y_matches, x_matches] = 1\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae8cc3ca-04fc-44d5-91b9-d2cbfad6a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    plt.pcolormesh(X_arr[x_min-1:x_max+1], Y_arr[y_min-1:y_max+1], template)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "970913b9-af5c-432a-9918-57f5165a3772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "try:\n",
    "\n",
    "    for i in tqdm(range(len(urls))):\n",
    "        design_matrices(i, mission, lamb, derivative, day_interval)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6798b2f-40d5-418f-aba7-a6937f914a1b",
   "metadata": {},
   "source": [
    "### Gather the dates for each datacube. They might not always correspond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e9253f0-0b28-4110-9c89-a868866399a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "try:\n",
    "    # Get the total amount of temporal steps\n",
    "    ind_tot = []\n",
    "    for i in urls:\n",
    "        ind_tot.append(data_dict[i]['dates'])\n",
    "    \n",
    "    ind_tot = np.unique(np.hstack(ind_tot))\n",
    "    \n",
    "    for i in urls:\n",
    "        data_dict[i]['ind_tot'] = np.array([np.where(c == ind_tot)[0][0] for c in data_dict[urls[0]]['dates']])\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2881fd9-41b1-48c2-95eb-f1f2f32d5ffe",
   "metadata": {},
   "source": [
    "### Calculate the point-inversion for all the GOI pixels in the datacubes for Vx and Vy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb2b67c6-bbe5-4997-9081-5d219cea955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if GPU:\n",
    "\n",
    "    import torch\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    '''\n",
    "    # Migrate to torch & GPU\n",
    "    for c in range(len(urls)):\n",
    "        data_dict[urls[c]]['A_m'] = torch.from_numpy(data_dict[urls[c]]['A_m']).to(device)\n",
    "        data_dict[urls[c]]['reg_mat_Inv'] = torch.from_numpy(data_dict[urls[c]]['reg_mat_Inv']).to(device)\n",
    "    '''\n",
    "    \n",
    "    def Inv_reg_torch(vObs, data, fillvalue, device):\n",
    "     \n",
    "        # Grab observed velocities\n",
    "        vObs = vObs[data['index_sort']]\n",
    "        vObs = vObs[data['mask_dates']]\n",
    "        \n",
    "        # Filter out the missions we don't want\n",
    "        if mission:\n",
    "            vObs = vObs[data['inds_mission']]  \n",
    "        \n",
    "        # Mask the NaNs so we don't compute the inversion for empty rows\n",
    "        mask = np.logical_not(np.equal(vObs, fillvalue))\n",
    "        \n",
    "        # Create a masked velocity vector\n",
    "        vObs_masked = torch.from_numpy(vObs[mask])\n",
    "        \n",
    "        # Append regularization terms to dObs\n",
    "        vObs_masked= torch.hstack((vObs_masked, torch.zeros((data['reg_mat_Inv'].shape[0])))).to(device)\n",
    "         \n",
    "        # Assemble the design matrix\n",
    "        A_des = torch.vstack((data['A_m'][mask], data['reg_mat_Inv']))\n",
    "        \n",
    "        # Invert the velocities\n",
    "        vInv = torch.linalg.solve(A_des.T@A_des,A_des.T@vObs_masked.double())\n",
    "        \n",
    "        return vInv\n",
    "    \n",
    "    vxInv = torch.zeros((len(ind_tot), template.shape[0], template.shape[1])).double().to(device)\n",
    "    vyInv = torch.zeros((vxInv.shape)).double().to(device)\n",
    "    \n",
    "    # Define the total number of iterations\n",
    "    total_iterations = len(y_matches)\n",
    "    \n",
    "    # Create a tqdm object with dynamic_ncols=False to suppress intermediate updates\n",
    "    # Create a tqdm object with a larger mininterval to suppress intermediate updates\n",
    "    progress_bar = tqdm(total=total_iterations, dynamic_ncols=False, mininterval=1.0)\n",
    "    \n",
    "    \n",
    "    i = 0\n",
    "    for c in range(len(urls)):\n",
    "        valid_idx = data_dict[urls[c]]['valid_idx']\n",
    "        fillvx = data_dict[urls[c]]['zarr_store']['vx'][:, valid_idx[0][0], valid_idx[1][0]].min() #data_dict[urls[c]]['zarr_store']['vx'].fill_value   fill_value is wrong in the new ITS_LIVE version\n",
    "        fillvy = data_dict[urls[c]]['zarr_store']['vx'][:, valid_idx[0][0], valid_idx[1][0]].min() #data_dict[urls[c]]['zarr_store']['vy'].fill_value\n",
    "    \n",
    "        for V in range(len(valid_idx[0])):\n",
    "            vxObs = data_dict[urls[c]]['zarr_store']['vx'][:, valid_idx[0][V], valid_idx[1][V]]\n",
    "            vyObs = data_dict[urls[c]]['zarr_store']['vy'][:, valid_idx[0][V], valid_idx[1][V]]\n",
    "            vxInv[data_dict[urls[c]]['ind_tot'], y_matches[i], x_matches[i]] = Inv_reg_torch(vxObs, data_dict[urls[c]], fillvx, device)\n",
    "            vyInv[data_dict[urls[c]]['ind_tot'], y_matches[i], x_matches[i]] = Inv_reg_torch(vyObs, data_dict[urls[c]], fillvy, device)\n",
    "            \n",
    "            i += 1\n",
    "            progress_bar.update(1)  # Update the progress bar\n",
    "\n",
    "            if i%100 == 0:\n",
    "                with open(\"Counter.txt\", \"w\") as text_file:\n",
    "                    text_file.write(f\"Counter: {i}\")\n",
    "            \n",
    "            if i%5000 == 0:\n",
    "                \n",
    "                print(f\"Saved at {i}\")\n",
    "                # Get the names of all the glaciers in the datacube \n",
    "                #glac_names = np.hstack(np.array([glacier.id.values[0] for glacier in gdf_list]))\n",
    "                #glac_names = '-'.join(glac_names)\n",
    "\n",
    "                # Gather the projection of the cube\n",
    "                #glac_proj = str(np.unique(np.hstack([data_dict[urls[i]]['proj_cube'] for i in range(len(urls))]))[0])\n",
    "\n",
    "                # Create a new dataset with vx and vy, using attributes from 'ds'\n",
    "                new_ds = xr.Dataset(\n",
    "                    {\n",
    "                        \"vx\": ([\"time\", \"y\", \"x\"], vxInv.cpu().numpy()),\n",
    "                        \"vy\": ([\"time\", \"y\", \"x\"], vyInv.cpu().numpy()),\n",
    "                    },\n",
    "                    coords={\n",
    "                        \"time\": ind_tot,\n",
    "                        \"x\": X_arr[x_min-1:x_max+1],\n",
    "                        \"y\": Y_arr[y_min-1:y_max+1],\n",
    "                    },\n",
    "                    attrs=data_dict[urls[0]]['zarr_store'].attrs,\n",
    "                ).chunk({'time': 1, 'x': 100, 'y': 100})\n",
    "\n",
    "                from dask.diagnostics import ProgressBar\n",
    "                write_job = new_ds.to_netcdf(f'Inverted_Cube.nc', compute=False)\n",
    "                with ProgressBar():\n",
    "                    print(f\"Writing to {'Inverted_Cube.nc'}\")\n",
    "                    write_job.compute()\n",
    "    \n",
    "    # Close the progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Save the dataset\n",
    "    new_ds = xr.Dataset(\n",
    "        {\n",
    "            \"vx\": ([\"time\", \"y\", \"x\"], vxInv.cpu().numpy()),\n",
    "            \"vy\": ([\"time\", \"y\", \"x\"], vyInv.cpu().numpy()),\n",
    "        },\n",
    "        coords={\n",
    "            \"time\": ind_tot,\n",
    "            \"x\": X_arr[x_min-1:x_max+1],\n",
    "            \"y\": Y_arr[y_min-1:y_max+1],\n",
    "        },\n",
    "        attrs=data_dict[urls[0]]['zarr_store'].attrs,\n",
    "    ).chunk({'time': 1, 'x': 100, 'y': 100})\n",
    "    \n",
    "    from dask.diagnostics import ProgressBar\n",
    "    write_job = new_ds.to_netcdf(f'VelInv.nc', compute=False)\n",
    "    with ProgressBar():\n",
    "        print(f\"Writing to {'VelInv.nc'}\")\n",
    "        write_job.compute()\n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "    \n",
    "    vxInv = np.zeros((len(ind_tot), template.shape[0], template.shape[1]))\n",
    "    vyInv = np.zeros((vxInv.shape))\n",
    "    \n",
    "    # Define the total number of iterations\n",
    "    total_iterations = len(y_matches)\n",
    "    \n",
    "    # Create a tqdm object with dynamic_ncols=False to suppress intermediate updates\n",
    "    # Create a tqdm object with a larger mininterval to suppress intermediate updates\n",
    "    progress_bar = tqdm(total=total_iterations, dynamic_ncols=False, mininterval=1.0)\n",
    "    \n",
    "    \n",
    "    i = 0\n",
    "    for c in range(len(urls)):\n",
    "        valid_idx = data_dict[urls[c]]['valid_idx']\n",
    "        fillvx = data_dict[urls[c]]['zarr_store']['vx'][:, valid_idx[0][0], valid_idx[1][0]].min() #data_dict[urls[c]]['zarr_store']['vx'].fill_value   fill_value is wrong in the new ITS_LIVE version\n",
    "        fillvy = data_dict[urls[c]]['zarr_store']['vx'][:, valid_idx[0][0], valid_idx[1][0]].min() #data_dict[urls[c]]['zarr_store']['vy'].fill_value\n",
    "        \n",
    "        for V in range(len(valid_idx[0])):\n",
    "            vxObs = data_dict[urls[c]]['zarr_store']['vx'][:, valid_idx[0][V], valid_idx[1][V]]\n",
    "            vyObs = data_dict[urls[c]]['zarr_store']['vy'][:, valid_idx[0][V], valid_idx[1][V]]\n",
    "            vxInv[data_dict[urls[c]]['ind_tot'], y_matches[i], x_matches[i]] = Inv_reg(vxObs, data_dict[urls[c]], fillvx)\n",
    "            vyInv[data_dict[urls[c]]['ind_tot'], y_matches[i], x_matches[i]] = Inv_reg(vyObs, data_dict[urls[c]], fillvy)\n",
    "            \n",
    "            i += 1\n",
    "            if i%100 == 0:\n",
    "                with open(\"Counter.txt\", \"w\") as text_file:\n",
    "                    text_file.write(f\"Counter: {i}\")\n",
    "            \n",
    "            if i%10000 == 0:\n",
    "                \n",
    "                print(f\"Saved at {i}\")\n",
    "                # Get the names of all the glaciers in the datacube \n",
    "                #glac_names = np.hstack(np.array([glacier.id.values[0] for glacier in gdf_list]))\n",
    "                #glac_names = '-'.join(glac_names)\n",
    "\n",
    "                # Gather the projection of the cube\n",
    "                #glac_proj = str(np.unique(np.hstack([data_dict[urls[i]]['proj_cube'] for i in range(len(urls))]))[0])\n",
    "\n",
    "                # Create a new dataset with vx and vy, using attributes from 'ds'\n",
    "                new_ds = xr.Dataset(\n",
    "                    {\n",
    "                        \"vx\": ([\"time\", \"y\", \"x\"], vxInv),\n",
    "                        \"vy\": ([\"time\", \"y\", \"x\"], vyInv),\n",
    "                    },\n",
    "                    coords={\n",
    "                        \"time\": ind_tot,\n",
    "                        \"x\": X_arr[x_min-1:x_max+1],\n",
    "                        \"y\": Y_arr[y_min-1:y_max+1],\n",
    "                    },\n",
    "                    attrs=data_dict[urls[0]]['zarr_store'].attrs,\n",
    "                ).chunk({'time': 1, 'x': 100, 'y': 100})\n",
    "\n",
    "                from dask.diagnostics import ProgressBar\n",
    "                write_job = new_ds.to_netcdf(f'Inverted_Cube.nc', compute=False)\n",
    "                with ProgressBar():\n",
    "                    print(f\"Writing to {'Inverted_Cube.nc'}\")\n",
    "                    write_job.compute()\n",
    "            progress_bar.update(1)  # Update the progress bar\n",
    "    \n",
    "    # Close the progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Save the dataset\n",
    "    new_ds = xr.Dataset(\n",
    "        {\n",
    "            \"vx\": ([\"time\", \"y\", \"x\"], vxInv),\n",
    "            \"vy\": ([\"time\", \"y\", \"x\"], vyInv),\n",
    "        },\n",
    "        coords={\n",
    "            \"time\": ind_tot,\n",
    "            \"x\": X_arr[x_min-1:x_max+1],\n",
    "            \"y\": Y_arr[y_min-1:y_max+1],\n",
    "        },\n",
    "        attrs=data_dict[urls[0]]['zarr_store'].attrs,\n",
    "    ).chunk({'time': 1, 'x': 100, 'y': 100})\n",
    "\n",
    "    from dask.diagnostics import ProgressBar\n",
    "    write_job = new_ds.to_netcdf(f'Inverted_Cube.nc', compute=False)\n",
    "    with ProgressBar():\n",
    "        print(f\"Writing to {'Inverted_Cube.nc'}\")\n",
    "        write_job.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737496d2-2194-4223-8295-4e0e4828f365",
   "metadata": {},
   "source": [
    "### Save the results in a netcdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dda34c8d-3abb-4d46-bab7-7e52c8ccd0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Create a new dataset with vx and vy, using attributes from 'ds'\n",
    "try:\n",
    "    new_ds = xr.Dataset(\n",
    "        {\n",
    "            \"vx\": ([\"time\", \"y\", \"x\"], vxInv),\n",
    "            \"vy\": ([\"time\", \"y\", \"x\"], vyInv),\n",
    "        },\n",
    "        coords={\n",
    "            \"time\": ind_tot,\n",
    "            \"x\": X_arr[x_min-1:x_max+1],\n",
    "            \"y\": Y_arr[y_min-1:y_max+1],\n",
    "        },\n",
    "        attrs=data_dict[urls[0]]['zarr_store'].attrs,\n",
    "    ).chunk({'time': 1, 'x': 100, 'y': 100})\n",
    "    \n",
    "    from dask.diagnostics import ProgressBar\n",
    "    write_job = new_ds.to_netcdf('Inverted_Cube.nc', compute=False)\n",
    "    with ProgressBar():\n",
    "        print(f\"Writing to {'Inverted_Cube.nc'}\")\n",
    "        write_job.compute()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9563817-8700-4ca5-9e33-71b3d1c34737",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    test = xr.open_dataset('Inverted_Cube.nc')\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
