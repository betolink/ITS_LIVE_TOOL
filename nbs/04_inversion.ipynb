{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: inversion.html\n",
    "title: '03: Velocity inversion'\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a76102-ce85-425b-918d-bd106a8f668d",
   "metadata": {},
   "source": [
    "# NOTE: still need to organize this code more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17710cd2-1523-4e14-9ed4-5d2f4e71573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp invert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9a30185-f150-4d86-b79b-9cdfea641c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev \n",
    "from nbdev import nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b430b2e-3bac-4b7d-bd00-926b77daf2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import pyproj\n",
    "import matplotlib.path as path\n",
    "import s3fs\n",
    "import zarr\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import re\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.path as mplp\n",
    "import ipyleaflet as ipyl\n",
    "from ipyleaflet import WMSLayer\n",
    "import ipywidgets as ipyw\n",
    "import json\n",
    "import pandas as pd\n",
    "from ipyleaflet import Map, WMSLayer, basemaps, GeoData\n",
    "from ipywidgets import HTML\n",
    "from owslib.wms import WebMapService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a731d171-1e95-4004-96c7-e8d021225caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from ITS_LIVE_TOOL import obj_setup, interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d35cdb3a-a8ae-46c2-873d-304f389f88cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f6704b-3838-4dad-88b5-5ce3c6190f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "data_map = interactive.Widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e8c3e89-7004-4bbe-a46f-5ff508e5504e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "802c2e3d26d4433f9080b5fb50a78ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Map(center=[0, 0], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_title', â€¦"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_map.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfbc1164-8c7c-4eea-86b2-0bf73f9c255f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_map.added_glaciers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d238bae-951f-4dec-8085-c1454eb96e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_map.added_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75e5dc53-1304-45a3-8544-ab21f8c1a3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://its-live-data.s3.amazonaws.com/datacubes/v2/N50W140/ITS_LIVE_vel_EPSG3413_G0120_X-3350000_Y350000.zarr',\n",
       " 'http://its-live-data.s3.amazonaws.com/datacubes/v2/N60W140/ITS_LIVE_vel_EPSG3413_G0120_X-3250000_Y350000.zarr',\n",
       " 'http://its-live-data.s3.amazonaws.com/datacubes/v2/N50W130/ITS_LIVE_vel_EPSG3413_G0120_X-3350000_Y250000.zarr',\n",
       " 'http://its-live-data.s3.amazonaws.com/datacubes/v2/N60W130/ITS_LIVE_vel_EPSG3413_G0120_X-3250000_Y250000.zarr']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_map.urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc6dc551-d5ba-4d11-bf7a-4e475230b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = data_map.added_coords\n",
    "unique_values, unique_indices = np.unique(np.array([i['id'] for i in data_map.added_glaciers]), return_index=True)\n",
    "gdf_list = [data_map.added_glaciers[i] for i in unique_indices]\n",
    "urls = list(set(data_map.urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b5ab3ae-c00a-439b-89f0-8ac95cb0e69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 2 glaciers selected\n"
     ]
    }
   ],
   "source": [
    "#|export\n",
    "try: \n",
    "    coords, gpdf, itslive_urls = obj_setup.return_clicked_info(data_map)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edc4a044-69d6-4f2e-8872-cae786aa88ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier0, glacier1 = obj_setup.create_multiple_glacier_objs(data_map)[0], obj_setup.create_multiple_glacier_objs(data_map)[1] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ea39a-c6ed-46c8-bd86-403a6b29c2e0",
   "metadata": {},
   "source": [
    "These glacier objects can be used to setup / feed data into the inversion and store inversion outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b1ba92b-69a6-46f0-bdea-b0a93039aec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://its-live-data.s3.amazonaws.com/datacubes/v2/N50W140/ITS_LIVE_vel_EPSG3413_G0120_X-3350000_Y350000.zarr'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glacier0.itslive_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e23c367-69e0-485a-ad28-da3db43cf06d",
   "metadata": {},
   "source": [
    "## Setup inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "816df992-9818-421c-b547-f601e5fdc45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mission = None # 'None' if you want all the data, 'S1' for Sentinel-1 only, 'L' for Landsat only, etc.. .\n",
    "lamb = 10 # Smoothing coefficient: the higher the value, the more the inversion favors a smooth output. BAD for surging glaciers, GOOD for non-surging glaciers.\n",
    "derivative = 2 # Derivative degree for the inversion. Doesn't change much unless you have a specific reasong to choose 1 or 2 (1st or 2nd derivative, Hessian of Jacobian)\n",
    "day_interval = 12 # Amount of days in between each inversion value. The higher, the faster the inversion. But you also lose in temporal resolution. 12 here because Sentinel-1 repeat-time is 12.\n",
    "sdate = None # Start date, format 'YYYY-MM-DD' OR None if you want to grab the entire timeseries available\n",
    "edate = None # End date, format 'YYYY-MM-DD' OR None if you want to grab the entire timeseries available\n",
    "GPU = True # None if you want to run it on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcf8b398-1051-4c9d-b503-e85937a622e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6e1c9a5-d834-43f8-80b0-678b6da47c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_ls = gpdf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae3dcf60-62f6-4729-9a2d-142c9bc5a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_input_dict(coords, gpdf, urls):\n",
    "    \n",
    "    mod_urls = [re.sub(r'http', 's3', url) for url in urls]\n",
    "    mod_urls = [re.sub(r'\\.s3\\.amazonaws\\.com', '', url) for url in mod_urls]\n",
    "    \n",
    "    d = {'coords': coords,\n",
    "     'gpdf': gpdf,\n",
    "     'urls': mod_urls\n",
    "        }\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a17e7b7-a8a0-43d7-9ff8-f7cc824bac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "try:\n",
    "    d = make_input_dict(coords, gpdf, urls)\n",
    "    urls = d['urls']\n",
    "except:\n",
    "    print(' no input data specified, have you made a map selection?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c82918ee-d3d7-4212-884b-5903e4319a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['coords', 'gpdf', 'urls'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ec6599a-be8b-48cd-926b-684d70c2a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_data_dict(urls):\n",
    "    # Modify the urls so they can be opened by zarr (replace 'http' by 's3' and delete '.s3.amazonaws.com')\n",
    "    mod_urls = [re.sub(r'http', 's3', url) for url in urls]\n",
    "    mod_urls = [re.sub(r'\\.s3\\.amazonaws\\.com', '', url) for url in urls]\n",
    "    \n",
    "    # Create storing arrays for the coordinates on-glacier\n",
    "    X_valid = []\n",
    "    Y_valid = []\n",
    "    X_tot = []\n",
    "    Y_tot = []\n",
    "    \n",
    "    # Create an empty directoryimport pickle to hold many variables all tied to the datacubes\n",
    "    data_dict = {}\n",
    "    \n",
    "\n",
    "    # We iterate through the different datacubes so they can each have one instance of the variables below\n",
    "    for mod_urls in urls:\n",
    "        zarr_store = None # To store the datacube's information and access its variables\n",
    "        dates = None # To store the dates at which the inversion will give values\n",
    "        A_m = None # 1st part of the design matrix\n",
    "        reg_mat_Inv = None # Regularization in time, 2nd part of the design matrix\n",
    "        mission = None # If you want to invert specifically for one mission in particular ('S1','L8','L9', etc...)\n",
    "        index_sort = None # Indices representing the sorted dates (from older to most recent)\n",
    "        inds_mission = None # Indices representing the sorted dates per mission chosen\n",
    "        ind_tot = None # Indices representing the indices of the pixels on the GOI\n",
    "        valid_idx = None # Easting and Northing values of the indices above\n",
    "        proj_cube = None # Projection of the datacube\n",
    "        mask_dates = None # Mask that filters out dates outside of desired date range\n",
    "        \n",
    "        # Create a dictionary entry for the URL with the desired subsets\n",
    "        data_dict[mod_urls] = {\n",
    "            'zarr_store': zarr_store,\n",
    "            'dates_noinv': dates,\n",
    "            'A_m': A_m,\n",
    "            'reg_mat_Inv': reg_mat_Inv,\n",
    "            'mission': mission,\n",
    "            'index_sort': index_sort,\n",
    "            'inds_mission': inds_mission,\n",
    "            'dates': dates,\n",
    "            'ind_tot': ind_tot,\n",
    "            'valid_idx': valid_idx,\n",
    "            'proj_cube': proj_cube,\n",
    "            'mask_dates': mask_dates\n",
    "        }\n",
    "        \n",
    "    return data_dict, X_valid, Y_valid, X_tot, Y_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f290802-70a5-4aac-89b9-16212d917107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def get_extents(url, gpdf, X_tot, Y_tot, X_valid, Y_valid, data_dict):# mission, lamb, derivative, day_interval):\n",
    "    \n",
    "    #url = input_data_dict['urls'].iloc[0]\n",
    "    \n",
    "    # Open the zarr files\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    store = zarr.open(s3fs.S3Map(url, s3=fs))\n",
    "   \n",
    "    # Update the dictionnary\n",
    "    data_dict[url]['zarr_store'] = store\n",
    "\n",
    "    # Get the cube's projection\n",
    "    proj_cube = store.attrs['projection']\n",
    "\n",
    "    # Load X and Y of the dataset\n",
    "    X = store['x'][:]\n",
    "    Y = store['y'][:]\n",
    "\n",
    "    # Store the arrays in the total list\n",
    "    X_tot.append(X)\n",
    "    Y_tot.append(Y)\n",
    "\n",
    "    # Load dimensions\n",
    "    shape_arr = store['v'].shape\n",
    "    \n",
    "    Xs, Ys = np.meshgrid(X, Y)\n",
    "    points = np.array((Xs.flatten(), Ys.flatten())).T\n",
    "\n",
    "    idx_valid = []\n",
    "    \n",
    "    for b in range(len(gpdf)):\n",
    "        mpath = mplp.Path(list(gpdf[b]['geometry'].to_crs(np.int(proj_cube)).boundary.explode(index_parts = True).iloc[0].coords))\n",
    "        glacier_mask = mpath.contains_points(points).reshape(Xs.shape)\n",
    "        # Grab the indices of the points inside the glacier\n",
    "        idx_valid.append(np.array(np.where(glacier_mask==True)))\n",
    "        \n",
    "    idx_valid = np.hstack(idx_valid)\n",
    "    # Store the valid indices\n",
    "    data_dict[url]['valid_idx'] = idx_valid\n",
    "    \n",
    "    # Store the cube projection\n",
    "    data_dict[url]['proj_cube'] = proj_cube\n",
    "    \n",
    "    # Store the coordinates of the valid Xs and Ys\n",
    "    X_valid.append([Xs[idx_valid[0][i], idx_valid[1][i]] for i in range(len(idx_valid[0]))])\n",
    "    Y_valid.append([Ys[idx_valid[0][i], idx_valid[1][i]] for i in range(len(idx_valid[0]))])\n",
    "    \n",
    "    return X_tot, Y_tot, X_valid, Y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d6bca53-ead5-4ed2-bc8c-4d981f2d1ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "#| export \n",
    "try:\n",
    "    data_dict, X_valid, Y_valid, X_tot, Y_tot = create_data_dict(urls)\n",
    "\n",
    "    for url in tqdm(range(len(urls))):\n",
    "        X_tot, Y_tot, X_valid, Y_valid = get_extents(urls[url], gpdf_ls, X_tot, Y_tot, X_valid, Y_valid, data_dict)\n",
    "\n",
    "    # Create Eastings and Northings arrays based on the Eastings and Northings of the datacubes\n",
    "    X_arr = np.unique(np.hstack(X_tot))\n",
    "    Y_arr = np.unique(np.hstack(Y_tot))\n",
    "    \n",
    "    # Crop to the GOI (so we avoid over-filling our matrix with NaNs)\n",
    "    x_min = np.where(np.min(np.hstack(X_valid)) == X_arr)[0][0]\n",
    "    x_max = np.where(np.max(np.hstack(X_valid)) == X_arr)[0][0]\n",
    "    y_min = np.where(np.min(np.hstack(Y_valid)) == Y_arr)[0][0]\n",
    "    y_max = np.where(np.max(np.hstack(Y_valid)) == Y_arr)[0][0]\n",
    "    \n",
    "    \n",
    "    # And now search the indices corresponding to the coordinates \n",
    "    x_matches = np.hstack([[np.where(i == X_arr[min(x_min-1, x_max+1):max(x_min-1, x_max+1)])[0][0] for i in row] for row in X_valid]).astype(int)\n",
    "    y_matches = np.hstack([[np.where(i == Y_arr[min(y_min-1, y_max+1):max(y_min-1, y_max+1)])[0][0] for i in row] for row in Y_valid]).astype(int)\n",
    "    \n",
    "    # Create an array representing the glacier\n",
    "    template = np.zeros((len(Y_arr[min(y_min-1, y_max+1):max(y_min-1, y_max+1)]), len( X_arr[min(x_min-1, x_max+1):max(x_min-1, x_max+1)])))\n",
    "    template[y_matches, x_matches] = 1\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01fa234e-9a5b-4138-ab37-4c2e920a8084",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    plt.pcolormesh(X_arr[x_min-1:x_max+1], Y_arr[y_min-1:y_max+1], template)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3795a667-ce66-4b2d-adab-03d5792f0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def design_matrices(url, mission, lamb, derivative, day_interval, sdate, edate):\n",
    "\n",
    "    # If you passed 'mission' as an argument, it grabs the appropriate values\n",
    "    if mission:\n",
    "        # Get the indices of the mission\n",
    "        filt1 = np.where(data_dict[urls[url]]['zarr_store']['satellite_img1'][:] == mission)\n",
    "        filt2 = np.where(data_dict[urls[url]]['zarr_store']['satellite_img2'][:] == mission)\n",
    "        inds_mission = np.intersect1d(filt1[0],filt2[0])\n",
    "\n",
    "        # Grab only the indices corresponding to the missions\n",
    "        mid_dates = np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['mid_date'][:], dtype='timedelta64[D]')[inds_mission]\n",
    "        im1 = np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['acquisition_date_img1'][:], dtype='timedelta64[D]')[inds_mission]\n",
    "        im2 = np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['acquisition_date_img2'][:], dtype='timedelta64[D]')[inds_mission]\n",
    "    else:\n",
    "        # If 'None' was passed as a mission argument, we grab all the available data.\n",
    "        inds_mission = None\n",
    "        mid_dates = np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['mid_date'][:], dtype='timedelta64[D]')\n",
    "        im1 = np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['acquisition_date_img1'][:], dtype='timedelta64[D]')\n",
    "        im2 = np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['acquisition_date_img2'][:], dtype='timedelta64[D]')\n",
    "    \n",
    "    # Get some arrays\n",
    "    index_sort = np.argsort(np.datetime64('1970-01-01') + np.array(data_dict[urls[url]]['zarr_store']['mid_date'][:], dtype='timedelta64[D]'))\n",
    "    mid_dates = mid_dates[index_sort]\n",
    "    im1 = im1[index_sort]\n",
    "    im2 = im2[index_sort]\n",
    "\n",
    "    # If sdate is later than the first available date, we find its corresponding index\n",
    "    try:\n",
    "        sdate_ind = np.where(mid_dates >= sdate)[0][0]\n",
    "    except:\n",
    "        sdate_ind = 0\n",
    "    \n",
    "    # If edate is sooner than the last available date, we find its corresponding index\n",
    "    try:\n",
    "        edate_ind = np.where(mid_dates > edate)[0][0]\n",
    "    except:\n",
    "        edate_ind = None\n",
    "    \n",
    "    # Create a False/True mask where True if the date is in the desired range\n",
    "    mask_dates = np.full(mid_dates.shape, False)\n",
    "    mask_dates[sdate_ind:edate_ind] = True\n",
    "\n",
    "    # Keep only the values within the desired range\n",
    "    mid_dates = mid_dates[mask_dates]\n",
    "    im1 = im1[mask_dates]\n",
    "    im2 = im2[mask_dates]\n",
    "\n",
    "    # Check which im is the smallest (first image, it changes depending on ITS_LIVE's version)\n",
    "    if im2[0] < im1[0]:\n",
    "        temp = im1\n",
    "        im1 = im2\n",
    "        im2 = temp\n",
    "\n",
    "    # Create the date array with the new interval dates\n",
    "    dates_nonum = np.arange(mid_dates[0], mid_dates[-1], timedelta(days=day_interval)).astype(np.datetime64)\n",
    "\n",
    "    # Convert to numerical\n",
    "    dates = (dates_nonum - np.datetime64('1970-01-01T00:00:00Z'))/np.timedelta64(1, 's')\n",
    "    dt_start = (im1 - np.datetime64('1970-01-01T00:00:00Z'))/np.timedelta64(1, 's')\n",
    "    dt_end = (im2 - np.datetime64('1970-01-01T00:00:00Z'))/np.timedelta64(1, 's')\n",
    "\n",
    "    # --------------- DESIGN MATRICES --------------- \n",
    "\n",
    "    # Initialize matrix\n",
    "    A_m = np.zeros((mid_dates.shape[0],dates.shape[0]))\n",
    "\n",
    "    # We have to iterate through the satellite pairs that actually gave a measurement\n",
    "    for j in range(1, len(mid_dates)):\n",
    "    # current contents of your for loop\n",
    "\n",
    "        # Find the middate that is the closest to dt_start (supequal)\n",
    "        start = np.argmin(np.abs(dates-dt_start[j]))\n",
    "\n",
    "        # Find the middate that is closest to dt_end (infequal)\n",
    "        end = np.argmin(dt_end[j] - dates[dates <= dt_end[j]])\n",
    "\n",
    "        # Divide 1 by the amount of middates between d_start and d_end \n",
    "        if end == A_m.shape[1]-1: # If the mid_date is at the end of the array (acquisition im2 equals last mid_date)\n",
    "            A_m[j, start:] = 1/(1+A_m.shape[1]-start)\n",
    "        else: # If the measurement is in A's bounds temporally (we can have a satellite pair with the 2nd pair being outside of our mid_dates)\n",
    "            A_m[j, start:end+1] = 1/(1+end-start) # Attribute to each pixel in the timescale of the satellite pair, the 1/amount of pixel in the pairmid_dates.shape\n",
    "\n",
    "\n",
    "    # Initialize regularization matrix\n",
    "    if derivative == 1:\n",
    "        reg_mat_Inv = np.zeros((A_m.shape[1] -1, A_m.shape[1]))\n",
    "\n",
    "        for j in range(A_m.shape[1] -1):\n",
    "            reg_mat_Inv[j, j] = -lamb/day_interval\n",
    "            reg_mat_Inv[j, j+1] = lamb/day_interval\n",
    "\n",
    "    elif derivative == 2:\n",
    "        # Initialize 2nd derivative regularization matrix\n",
    "        reg_mat_Inv = np.zeros((A_m.shape[1] -1, A_m.shape[1]))\n",
    "\n",
    "        for j in range(A_m.shape[1] -2):\n",
    "            reg_mat_Inv[j, j] = lamb/(day_interval**2)\n",
    "            reg_mat_Inv[j, j+1] = -2*lamb/(day_interval**2)\n",
    "            reg_mat_Inv[j, j+2] = lamb/(day_interval**2)\n",
    "            \n",
    "    data_dict[urls[url]]['A_m'] = A_m\n",
    "    data_dict[urls[url]]['reg_mat_Inv'] = reg_mat_Inv\n",
    "    data_dict[urls[url]]['mission'] = mission\n",
    "    data_dict[urls[url]]['index_sort'] = index_sort\n",
    "    data_dict[urls[url]]['inds_mission'] = inds_mission\n",
    "    data_dict[urls[url]]['dates'] = dates_nonum\n",
    "    data_dict[urls[url]]['dates_noinv'] = mid_dates\n",
    "    data_dict[urls[url]]['mask_dates']= mask_dates\n",
    "            \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3b6cf86-b960-4e19-af87-bf4cf9207026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "try:\n",
    "\n",
    "    for i in tqdm(range(len(urls))):\n",
    "        design_matrices(i, mission, lamb, derivative, day_interval)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4f911d0-9082-43ff-9f74-0cdcc0783c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def Inv_reg(vObs, data, fillvalue):\n",
    "    \n",
    "    # Grab observed velocities\n",
    "    vObs = vObs[data['index_sort']]\n",
    "    vObs = vObs[data['mask_dates']]\n",
    "    \n",
    "    # Filter out the missions we don't want\n",
    "    if mission:\n",
    "        vObs = vObs[data['inds_mission']]  \n",
    "    \n",
    "    # Mask the NaNs so we don't compute the inversion for empty rows\n",
    "    mask = np.logical_not(np.equal(vObs, fillvalue))\n",
    "    \n",
    "    # Create a masked velocity vector\n",
    "    vObs_masked = vObs[mask]\n",
    "    \n",
    "    # Append regularization terms to dObs\n",
    "    vObs_masked= np.hstack((vObs_masked, np.zeros((data['reg_mat_Inv'].shape[0]))))\n",
    "     \n",
    "    # Assemble the design matrix\n",
    "    A_des = np.vstack((data['A_m'][mask], data['reg_mat_Inv']))\n",
    "    \n",
    "    # Invert the velocities\n",
    "    vInv = np.linalg.solve(A_des.T@A_des,A_des.T@vObs_masked)\n",
    "    #vInv = scipy.linalg.solve(A_des.T@A_des,A_des.T@vObs_masked, lower = False, check_finite=False, assume_a='gen')\n",
    "    \n",
    "    return vInv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a1ff31a-e46d-49c3-a8f7-d03701a02e89",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m urls:\n\u001b[1;32m      4\u001b[0m     ind_tot\u001b[38;5;241m.\u001b[39mappend(data_dict[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdates\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m ind_tot \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(np\u001b[38;5;241m.\u001b[39mhstack(ind_tot))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m urls:\n\u001b[1;32m      9\u001b[0m     data_dict[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mind_tot\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mwhere(c \u001b[38;5;241m==\u001b[39m ind_tot)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m data_dict[urls[\u001b[38;5;241m0\u001b[39m]][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdates\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/software/pkg/miniconda3/envs/itslivetools_env/lib/python3.11/site-packages/numpy/core/shape_base.py:345\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "# Get the total amount of temporal steps\n",
    "ind_tot = []\n",
    "for i in urls:\n",
    "    ind_tot.append(data_dict[i]['dates'])\n",
    "\n",
    "ind_tot = np.unique(np.hstack(ind_tot))\n",
    "\n",
    "for i in urls:\n",
    "    data_dict[i]['ind_tot'] = np.array([np.where(c == ind_tot)[0][0] for c in data_dict[urls[0]]['dates']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a81a0d7-d627-4398-bc56-aca641e99786",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m GPU:\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    # Migrate to torch & GPU\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    for c in range(len(urls)):\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m        data_dict[urls[c]]['A_m'] = torch.from_numpy(data_dict[urls[c]]['A_m']).to(device)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m        data_dict[urls[c]]['reg_mat_Inv'] = torch.from_numpy(data_dict[urls[c]]['reg_mat_Inv']).to(device)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n",
      "File \u001b[0;32m~/software/pkg/miniconda3/envs/itslivetools_env/lib/python3.11/site-packages/torch/__init__.py:235\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    234\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if GPU:\n",
    "\n",
    "    import torch\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    '''\n",
    "    # Migrate to torch & GPU\n",
    "    for c in range(len(urls)):\n",
    "        data_dict[urls[c]]['A_m'] = torch.from_numpy(data_dict[urls[c]]['A_m']).to(device)\n",
    "        data_dict[urls[c]]['reg_mat_Inv'] = torch.from_numpy(data_dict[urls[c]]['reg_mat_Inv']).to(device)\n",
    "    '''\n",
    "    \n",
    "    def Inv_reg_torch(vObs, data, fillvalue, device):\n",
    "     \n",
    "        # Grab observed velocities\n",
    "        vObs = vObs[data['index_sort']]\n",
    "        vObs = vObs[data['mask_dates']]\n",
    "        \n",
    "        # Filter out the missions we don't want\n",
    "        if mission:\n",
    "            vObs = vObs[data['inds_mission']]  \n",
    "        \n",
    "        # Mask the NaNs so we don't compute the inversion for empty rows\n",
    "        mask = np.logical_not(np.equal(vObs, fillvalue))\n",
    "        \n",
    "        # Create a masked velocity vector\n",
    "        vObs_masked = torch.from_numpy(vObs[mask])\n",
    "        \n",
    "        # Append regularization terms to dObs\n",
    "        vObs_masked= torch.hstack((vObs_masked, torch.zeros((data['reg_mat_Inv'].shape[0])))).to(device)\n",
    "         \n",
    "        # Assemble the design matrix\n",
    "        A_des = torch.vstack((data['A_m'][mask], data['reg_mat_Inv']))\n",
    "        \n",
    "        # Invert the velocities\n",
    "        vInv = torch.linalg.solve(A_des.T@A_des,A_des.T@vObs_masked.double())\n",
    "        \n",
    "        return vInv\n",
    "    \n",
    "    vxInv = torch.zeros((len(ind_tot), template.shape[0], template.shape[1])).double().to(device)\n",
    "    vyInv = torch.zeros((vxInv.shape)).double().to(device)\n",
    "    \n",
    "    # Define the total number of iterations\n",
    "    total_iterations = len(y_matches)\n",
    "    \n",
    "    # Create a tqdm object with dynamic_ncols=False to suppress intermediate updates\n",
    "    # Create a tqdm object with a larger mininterval to suppress intermediate updates\n",
    "    progress_bar = tqdm(total=total_iterations, dynamic_ncols=False, mininterval=1.0)\n",
    "    \n",
    "    \n",
    "    i = 0\n",
    "    for c in range(len(urls)):\n",
    "        valid_idx = data_dict[urls[c]]['valid_idx']\n",
    "        fillvx = data_dict[urls[c]]['zarr_store']['vx'][:, valid_idx[0][0], valid_idx[1][0]].min() #data_dict[urls[c]]['zarr_store']['vx'].fill_value   fill_value is wrong in the new ITS_LIVE version\n",
    "        fillvy = data_dict[urls[c]]['zarr_store']['vx'][:, valid_idx[0][0], valid_idx[1][0]].min() #data_dict[urls[c]]['zarr_store']['vy'].fill_value\n",
    "    \n",
    "        for V in range(len(valid_idx[0])):\n",
    "            vxObs = data_dict[urls[c]]['zarr_store']['vx'][:, valid_idx[0][V], valid_idx[1][V]]\n",
    "            vyObs = data_dict[urls[c]]['zarr_store']['vy'][:, valid_idx[0][V], valid_idx[1][V]]\n",
    "            vxInv[data_dict[urls[c]]['ind_tot'], y_matches[i], x_matches[i]] = Inv_reg_torch(vxObs, data_dict[urls[c]], fillvx, device)\n",
    "            vyInv[data_dict[urls[c]]['ind_tot'], y_matches[i], x_matches[i]] = Inv_reg_torch(vyObs, data_dict[urls[c]], fillvy, device)\n",
    "            \n",
    "            i += 1\n",
    "            progress_bar.update(1)  # Update the progress bar\n",
    "\n",
    "            if i%100 == 0:\n",
    "                with open(\"Counter.txt\", \"w\") as text_file:\n",
    "                    text_file.write(f\"Counter: {i}\")\n",
    "            \n",
    "            if i%5000 == 0:\n",
    "                \n",
    "                print(f\"Saved at {i}\")\n",
    "                # Get the names of all the glaciers in the datacube \n",
    "                #glac_names = np.hstack(np.array([glacier.id.values[0] for glacier in gdf_list]))\n",
    "                #glac_names = '-'.join(glac_names)\n",
    "\n",
    "                # Gather the projection of the cube\n",
    "                #glac_proj = str(np.unique(np.hstack([data_dict[urls[i]]['proj_cube'] for i in range(len(urls))]))[0])\n",
    "\n",
    "                # Create a new dataset with vx and vy, using attributes from 'ds'\n",
    "                new_ds = xr.Dataset(\n",
    "                    {\n",
    "                        \"vx\": ([\"time\", \"y\", \"x\"], vxInv.cpu().numpy()),\n",
    "                        \"vy\": ([\"time\", \"y\", \"x\"], vyInv.cpu().numpy()),\n",
    "                    },\n",
    "                    coords={\n",
    "                        \"time\": ind_tot,\n",
    "                        \"x\": X_arr[x_min-1:x_max+1],\n",
    "                        \"y\": Y_arr[y_min-1:y_max+1],\n",
    "                    },\n",
    "                    attrs=data_dict[urls[0]]['zarr_store'].attrs,\n",
    "                ).chunk({'time': 1, 'x': 100, 'y': 100})\n",
    "\n",
    "                from dask.diagnostics import ProgressBar\n",
    "                write_job = new_ds.to_netcdf(f'VelInv.nc', compute=False)\n",
    "                with ProgressBar():\n",
    "                    print(f\"Writing to {'VelInv.nc'}\")\n",
    "                    write_job.compute()\n",
    "    \n",
    "    # Close the progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Save the dataset\n",
    "    new_ds = xr.Dataset(\n",
    "        {\n",
    "            \"vx\": ([\"time\", \"y\", \"x\"], vxInv.cpu().numpy()),\n",
    "            \"vy\": ([\"time\", \"y\", \"x\"], vyInv.cpu().numpy()),\n",
    "        },\n",
    "        coords={\n",
    "            \"time\": ind_tot,\n",
    "            \"x\": X_arr[x_min-1:x_max+1],\n",
    "            \"y\": Y_arr[y_min-1:y_max+1],\n",
    "        },\n",
    "        attrs=data_dict[urls[0]]['zarr_store'].attrs,\n",
    "    ).chunk({'time': 1, 'x': 100, 'y': 100})\n",
    "    \n",
    "    from dask.diagnostics import ProgressBar\n",
    "    write_job = new_ds.to_netcdf(f'VelInv.nc', compute=False)\n",
    "    with ProgressBar():\n",
    "        print(f\"Writing to {'VelInv.nc'}\")\n",
    "        write_job.compute()\n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "    \n",
    "    vxInv = np.zeros((len(ind_tot), template.shape[0], template.shape[1]))\n",
    "    vyInv = np.zeros((vxInv.shape))\n",
    "    \n",
    "    # Define the total number of iterations\n",
    "    total_iterations = len(y_matches)\n",
    "    \n",
    "    # Create a tqdm object with dynamic_ncols=False to suppress intermediate updates\n",
    "    # Create a tqdm object with a larger mininterval to suppress intermediate updates\n",
    "    progress_bar = tqdm(total=total_iterations, dynamic_ncols=False, mininterval=1.0)\n",
    "    \n",
    "    \n",
    "    i = 0\n",
    "    for c in range(len(urls)):\n",
    "        valid_idx = data_dict[urls[c]]['valid_idx']\n",
    "        fillvx = data_dict[urls[c]]['zarr_store']['vx'][:, valid_idx[0][0], valid_idx[1][0]].min() #data_dict[urls[c]]['zarr_store']['vx'].fill_value   fill_value is wrong in the new ITS_LIVE version\n",
    "        fillvy = data_dict[urls[c]]['zarr_store']['vx'][:, valid_idx[0][0], valid_idx[1][0]].min() #data_dict[urls[c]]['zarr_store']['vy'].fill_value\n",
    "        \n",
    "        for V in range(len(valid_idx[0])):\n",
    "            vxObs = data_dict[urls[c]]['zarr_store']['vx'][:, valid_idx[0][V], valid_idx[1][V]]\n",
    "            vyObs = data_dict[urls[c]]['zarr_store']['vy'][:, valid_idx[0][V], valid_idx[1][V]]\n",
    "            vxInv[data_dict[urls[c]]['ind_tot'], y_matches[i], x_matches[i]] = Inv_reg(vxObs, data_dict[urls[c]], fillvx)\n",
    "            vyInv[data_dict[urls[c]]['ind_tot'], y_matches[i], x_matches[i]] = Inv_reg(vyObs, data_dict[urls[c]], fillvy)\n",
    "            \n",
    "            i += 1\n",
    "            if i%100 == 0:\n",
    "                with open(\"Counter.txt\", \"w\") as text_file:\n",
    "                    text_file.write(f\"Counter: {i}\")\n",
    "            \n",
    "            if i%10000 == 0:\n",
    "                \n",
    "                print(f\"Saved at {i}\")\n",
    "                # Get the names of all the glaciers in the datacube \n",
    "                #glac_names = np.hstack(np.array([glacier.id.values[0] for glacier in gdf_list]))\n",
    "                #glac_names = '-'.join(glac_names)\n",
    "\n",
    "                # Gather the projection of the cube\n",
    "                #glac_proj = str(np.unique(np.hstack([data_dict[urls[i]]['proj_cube'] for i in range(len(urls))]))[0])\n",
    "\n",
    "                # Create a new dataset with vx and vy, using attributes from 'ds'\n",
    "                new_ds = xr.Dataset(\n",
    "                    {\n",
    "                        \"vx\": ([\"time\", \"y\", \"x\"], vxInv),\n",
    "                        \"vy\": ([\"time\", \"y\", \"x\"], vyInv),\n",
    "                    },\n",
    "                    coords={\n",
    "                        \"time\": ind_tot,\n",
    "                        \"x\": X_arr[x_min-1:x_max+1],\n",
    "                        \"y\": Y_arr[y_min-1:y_max+1],\n",
    "                    },\n",
    "                    attrs=data_dict[urls[0]]['zarr_store'].attrs,\n",
    "                ).chunk({'time': 1, 'x': 100, 'y': 100})\n",
    "\n",
    "                from dask.diagnostics import ProgressBar\n",
    "                write_job = new_ds.to_netcdf(f'Malaspina.nc', compute=False)\n",
    "                with ProgressBar():\n",
    "                    print(f\"Writing to {'Malaspina.nc'}\")\n",
    "                    write_job.compute()\n",
    "            progress_bar.update(1)  # Update the progress bar\n",
    "    \n",
    "    # Close the progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Save the dataset\n",
    "    new_ds = xr.Dataset(\n",
    "        {\n",
    "            \"vx\": ([\"time\", \"y\", \"x\"], vxInv),\n",
    "            \"vy\": ([\"time\", \"y\", \"x\"], vyInv),\n",
    "        },\n",
    "        coords={\n",
    "            \"time\": ind_tot,\n",
    "            \"x\": X_arr[x_min-1:x_max+1],\n",
    "            \"y\": Y_arr[y_min-1:y_max+1],\n",
    "        },\n",
    "        attrs=data_dict[urls[0]]['zarr_store'].attrs,\n",
    "    ).chunk({'time': 1, 'x': 100, 'y': 100})\n",
    "\n",
    "    from dask.diagnostics import ProgressBar\n",
    "    write_job = new_ds.to_netcdf(f'Malaspina.nc', compute=False)\n",
    "    with ProgressBar():\n",
    "        print(f\"Writing to {'Malaspina.nc'}\")\n",
    "        write_job.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da08082-0feb-4f64-b21a-053a2f6e1614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
