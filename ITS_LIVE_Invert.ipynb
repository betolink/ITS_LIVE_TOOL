{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f2425ee-b75f-4115-a1a3-0b472e0f0414",
   "metadata": {},
   "source": [
    "**Import the data from ITS_LIVE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8bb9e82-ed24-4b59-a29c-0a78ad388c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cube....\n",
      "Loading cube....\n",
      "Loading cube....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/envs/glaciervel/lib/python3.11/site-packages/xarray/core/indexing.py:1374: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cube....\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import xarray as xr\n",
    "import os\n",
    "import sys\n",
    "from dask.diagnostics import ProgressBar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Select Threshold quality (higher the threshold, the least data has to be downloaded)\n",
    "threshold = 0\n",
    "\n",
    "sdate = '2022-01-01'\n",
    "edate = '2025-04-31'\n",
    "cubes = ['http://its-live-data.s3.amazonaws.com/datacubes/v02/N60W140/ITS_LIVE_vel_EPSG3413_G0120_X-3250000_Y350000.zarr',\n",
    " 'http://its-live-data.s3.amazonaws.com/datacubes/v02/N50W130/ITS_LIVE_vel_EPSG3413_G0120_X-3350000_Y250000.zarr',\n",
    " 'http://its-live-data.s3.amazonaws.com/datacubes/v02/N60W130/ITS_LIVE_vel_EPSG3413_G0120_X-3250000_Y250000.zarr',\n",
    " 'http://its-live-data.s3.amazonaws.com/datacubes/v02/N50W140/ITS_LIVE_vel_EPSG3413_G0120_X-3350000_Y350000.zarr']\n",
    "\n",
    "xmin =-3340199.841439601\n",
    "xmax =-3266124.863581888\n",
    "ymin =273869.3657119706\n",
    "ymax =364857.29614944384\n",
    "\n",
    "\n",
    "\n",
    "# Create path to the files\n",
    "pathsave = 'Datacubes/Subyearly/Alaska/'\n",
    "os.makedirs(pathsave, exist_ok = True)\n",
    "variables_to_keep = ['v', 'date_dt', 'acquisition_date_img1', 'acquisition_date_img2']\n",
    "\n",
    "def load_datacubes(datacube_address, sdate, edate, xmax, xmin, ymax, ymin, variables_drop):\n",
    "\n",
    "    print('Loading cube....')\n",
    "    # Grab the time values\n",
    "    t = xr.open_dataset(datacube_address, engine='zarr').mid_date.values\n",
    "    # Load indices of slices above the quality threshold\n",
    "    valid = xr.open_dataset(datacube_address, engine='zarr').roi_valid_percentage.values\n",
    "\n",
    "    # Create a time mask, based on the validity of layers and the custom date-range\n",
    "    t_mask = np.logical_and(valid>threshold, np.logical_and(t>np.datetime64(sdate), t<np.datetime64(edate)))\n",
    "                    \n",
    "\n",
    "    # Load datacube according to prerequisites (time, space and variables)\n",
    "    ds = xr.open_zarr(datacube_address,\n",
    "                chunks=({'mid_date': -1, 'y': 100, 'x': 100}),\n",
    "                drop_variables=variables_drop\n",
    "                ).sel(  mid_date = t_mask,\n",
    "                        x=slice(xmin, xmax),\n",
    "                        y=slice(ymax, ymin)).drop_duplicates(dim='mid_date')\n",
    "    ds.attrs={}\n",
    "    return ds\n",
    "\n",
    "variables_to_keep += ['mid_date', 'x', 'y']\n",
    "\n",
    "# List of variables to drop for the download (we drop everything but the variables written below)\n",
    "variables_drop = [ele for ele in list(\n",
    "        xr.open_dataset(cubes[0], engine='zarr').variables\n",
    "        ) if ele not in variables_to_keep\n",
    "]\n",
    "\n",
    "datacubes = [load_datacubes(datacube_address, sdate, edate, xmax, xmin, ymax, ymin, variables_drop).sortby(['mid_date']) for datacube_address in cubes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8c4f6c-38a7-4687-8ab3-e350b4073d50",
   "metadata": {},
   "source": [
    "**Define the inverse function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9836371b-2335-4c98-ada7-aa635a2b1060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function inputs a 1D tensor, aka a pixel from the 3D ITS_LIVE tensor. It returns a smaller 1D tensor\n",
    "def invert_vel(pt):\n",
    "\n",
    "    # get indices of non_nan values in the timeseries\n",
    "    non_nan_idx = np.flatnonzero(~np.isnan(pt))\n",
    "\n",
    "    # Calculate the length once to avoid repeated calculations\n",
    "    non_nan_len = len(non_nan_idx)\n",
    "\n",
    "    # Pickup the dates that don't have NaNs\n",
    "    dt_start_non_nan = dt_start[non_nan_idx]\n",
    "    dt_end_non_nan = dt_end[non_nan_idx]\n",
    "\n",
    "    # Initialize matrix\n",
    "    A = np.zeros((non_nan_len, dates.shape[0]))\n",
    "\n",
    "    # Find the indices for dt_start and dt_end in the dates array\n",
    "    start_indices = np.searchsorted(dates, dt_start_non_nan)\n",
    "    end_indices = np.searchsorted(dates, dt_end_non_nan, side='right') - 1\n",
    "\n",
    "    # Fill-in the design matrix \n",
    "    for j in range(non_nan_len):\n",
    "        start = start_indices[j]\n",
    "        end = end_indices[j]\n",
    "\n",
    "        if end == A.shape[1] - 2:  # If the mid_date is at the end of the array (acquisition im2 equals last mid_date)\n",
    "            A[j, start:] = 1 / (1 + A.shape[1] - start)\n",
    "        else:  # If the measurement is in A's bounds temporally\n",
    "            A[j, start:end + 1] = 1 / (1 + end - start)\n",
    "\n",
    "    # --------------- INVERSION --------------- #\n",
    "\n",
    "    # Calculate the weights and norms\n",
    "    M = A.shape[1]\n",
    "\n",
    "    # Simpson rule weights\n",
    "    dg = np.ones(M) * 1 / 3\n",
    "    dg[1:M - 1] += 1 / 3\n",
    "    dg[1:M - 2:2] += 2 / 3\n",
    "    W = np.sqrt(np.diag(dg))\n",
    "\n",
    "    # Differencing matrix\n",
    "    delta = np.diag(np.ones(M)) - np.diag(np.ones(M - 1), -1)\n",
    "    delta[0, 0] = 0\n",
    "\n",
    "    # Roughness norm\n",
    "    R = W @ delta\n",
    "\n",
    "    # Expression from differencing the minimizing functional with respect to v_i\n",
    "    LHS = A.T @ A + 1 / lamb * R.T @ R\n",
    "\n",
    "    # Retrieve velocities\n",
    "    po, res = nnls(LHS, A.T @ pt[non_nan_idx])\n",
    "    \n",
    "    return po\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "797da3f1-6412-4bf1-89da-970b83cec37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy.sparse as sp\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from scipy.optimize import nnls\n",
    "import time\n",
    "from datetime import timedelta, date\n",
    "\n",
    "# Extract the important variables from the datacube and create the template time-interval on which the inverse function will be applied to\n",
    "def build_output_arrays(dataset, day_interval):\n",
    "    \n",
    "    # Round all time arrays to days, because it doesn't make sense to have nanoseconds\n",
    "    mid_date = dataset.mid_date.values\n",
    "    \n",
    "    # Create the date array with the new interval dates\n",
    "    dates_nonum = np.arange(mid_date[0], mid_date[-1], timedelta(days=day_interval),dtype='datetime64[ns]')\n",
    "\n",
    "    # Convert to numerical\n",
    "    dates = pd.to_numeric(dates_nonum)\n",
    "    dt_start = pd.to_numeric(dataset.acquisition_date_img1.values)\n",
    "    dt_end = pd.to_numeric(dataset.acquisition_date_img2.values)\n",
    "\n",
    "    \n",
    "    return dt_start, dt_end, dates, dates_nonum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae854ef-72b4-44a9-8e05-e50de5ff08e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing datacube 1 of 4\n",
      "[########################################] | 100% Completed | 114m 28s\n",
      "Computing datacube 2 of 4\n",
      "[#                                       ] | 3% Completed | 3.23 s ms"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_154/262566881.py:29: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  A[j, start:end + 1] = 1 / (1 + end - start)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 50m 3ss\n",
      "Computing datacube 3 of 4\n",
      "[##                                      ] | 5% Completed | 8.34 s ms"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_154/262566881.py:29: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  A[j, start:end + 1] = 1 / (1 + end - start)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 31m 1ss\n",
      "Computing datacube 4 of 4\n",
      "[#############                           ] | 34% Completed | 13m 3sss"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_154/262566881.py:29: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  A[j, start:end + 1] = 1 / (1 + end - start)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[##########################              ] | 65% Completed | 58m 42s"
     ]
    }
   ],
   "source": [
    "# List to store the inverted datacubes\n",
    "inverted_cubes = []\n",
    "\n",
    "# For each datacube:\n",
    "for i in range(len(datacubes)):\n",
    "    # Print the statement before each datacube computation\n",
    "    print(f\"Computing datacube {i+1} of {len(datacubes)}\")\n",
    "    \n",
    "    # Grab the important variables and create the inverse timeframe\n",
    "    dt_start, dt_end, dates, dates_nonum = build_output_arrays(datacubes[i], 5)\n",
    "    lamb = 5\n",
    "    \n",
    "    # Apply the function using apply_ufunc\n",
    "    output_data = xr.apply_ufunc(invert_vel, datacubes[i].v,\n",
    "                                 vectorize=True,\n",
    "                                 input_core_dims=[['mid_date']],\n",
    "                                 output_core_dims=[['time']],\n",
    "                                 dask='parallelized',\n",
    "                                 output_dtypes=[float],\n",
    "                                 dask_gufunc_kwargs={'output_sizes': {'time': len(dates)}})\n",
    "    \n",
    "    # Run in parallel for each chunk the inverse function\n",
    "    with ProgressBar():\n",
    "        cube = output_data.compute().assign_coords(\n",
    "                time=dates_nonum, dims='time').transpose(\n",
    "                'time', 'y', 'x').chunk(\n",
    "                    {'time':-1, 'y':100,'x':100})\n",
    "        \n",
    "        # Store the results in the list\n",
    "        inverted_cubes.append(cube.reindex(y=cube['y'], x=cube['x'], time=cube['time']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92131fe3-b47b-4365-8ba5-5a58d5792d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "# Combine the data arrays using `combine_first()`\n",
    "combined_data = functools.reduce(lambda a, b: a.combine_first(b), inverted_cubes).chunk({'time':-1, 'y':100,'x':100})\n",
    "\n",
    "write_job = combined_data.to_netcdf(\"/home/jovyan/ITS-LIVE-Downloader-Tracker/test.nc\",compute=False)\n",
    "with ProgressBar():\n",
    "    print(f\"Writing to {pathsave}\")\n",
    "    write_job = write_job.compute()  \n",
    "    \n",
    "test = xr.open_dataset(f'/home/jovyan/ITS-LIVE-Downloader-Tracker/test_{threshold}.nc')\n",
    "plt.figure()\n",
    "plt.imshow(np.nanmean(test.v.values, axis = 0), origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7837bf3f-7584-4425-93af-d5ee0127c9a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glaciervel [conda env:.local-glaciervel]",
   "language": "python",
   "name": "conda-env-.local-glaciervel-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
